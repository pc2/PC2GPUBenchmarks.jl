var documenterSearchIndex = {"docs":
[{"location":"refs/gpustresstest/#GPU-Stresstest","page":"GPU Stress Test","title":"GPU Stresstest","text":"","category":"section"},{"location":"refs/gpustresstest/#Index","page":"GPU Stress Test","title":"Index","text":"","category":"section"},{"location":"refs/gpustresstest/","page":"GPU Stress Test","title":"GPU Stress Test","text":"Pages   = [\"gpustresstest.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/gpustresstest/#References","page":"GPU Stress Test","title":"References","text":"","category":"section"},{"location":"refs/gpustresstest/","page":"GPU Stress Test","title":"GPU Stress Test","text":"Modules = [PC2GPUBenchmarks]\nPages   = [\"stresstest.jl\", \"stresstest_tests.jl\"]","category":"page"},{"location":"refs/gpustresstest/#PC2GPUBenchmarks.stresstest-Tuple{Any}","page":"GPU Stress Test","title":"PC2GPUBenchmarks.stresstest","text":"stresstest(device_or_devices)\n\nRun a GPU stress test (matrix multiplication) on one or multiple GPU devices, as specified by the positional argument. If no argument is provided (only) the currently active GPU will be used.\n\nKeyword arguments:\n\nChoose one of the following (or none):\n\nduration: stress test will take about the given time in seconds. (StressTestBatched)\nenforced_duration: stress test will take almost precisely the given time in seconds. (StressTestEnforced)\napprox_duration: stress test will hopefully take approximately the given time in seconds. No promises made! (StressTestFixedIter)\nniter: stress test will run the given number of matrix-multiplications, however long that will take. (StressTestFixedIter)\nmem: number (<:Real) between 0 and 1, indicating the fraction of the available GPU memory that should be used, or a <:UnitPrefixedBytes indicating an absolute memory limit. (StressTestStoreResults)\n\nGeneral settings:\n\ndtype (default: Float32): element type of the matrices\nsize (default: 2048): matrices of size (size, size) will be used\nverbose (default: true): toggle printing of information\nparallel (default: true): If true, will (try to) run each GPU test on a different Julia thread. Make sure to have enough Julia threads.\nthreads (default: nothing): If parallel == true, this argument may be used to specify the Julia threads to use.\nclearmem (default: false): If true, we call clear_all_gpus_memory after the stress test.\n\nWhen duration is specifiec (i.e. StressTestEnforced) there is also:\n\nbatch_duration (default: ceil(Int, duration/10)): desired duration of one batch of matmuls.\n\n\n\n\n\n","category":"method"},{"location":"refs/gpustresstest/#PC2GPUBenchmarks.StressTestBatched","page":"GPU Stress Test","title":"PC2GPUBenchmarks.StressTestBatched","text":"GPU stress test (matrix multiplications) in which we try to run for a given time period. We try to keep the CUDA stream continously busy with matmuls at any point in time. Concretely, we submit batches of matmuls and, after half of them, we record a CUDA event. On the host, after submitting a batch, we (non-blockingly) synchronize on, i.e. wait for, the CUDA event and, if we haven't exceeded the desired duration already, submit another batch.\n\n\n\n\n\n","category":"type"},{"location":"refs/gpustresstest/#PC2GPUBenchmarks.StressTestEnforced","page":"GPU Stress Test","title":"PC2GPUBenchmarks.StressTestEnforced","text":"GPU stress test (matrix multiplications) in which we run almost precisely for a given time period (duration is enforced).\n\n\n\n\n\n","category":"type"},{"location":"refs/gpustresstest/#PC2GPUBenchmarks.StressTestFixedIter","page":"GPU Stress Test","title":"PC2GPUBenchmarks.StressTestFixedIter","text":"GPU stress test (matrix multiplications) in which we run for a given number of iteration, or try to run for a given time period (with potentially high uncertainty!). In the latter case, we estimate how long a synced matmul takes and set niter accordingly.\n\n\n\n\n\n","category":"type"},{"location":"refs/gpustresstest/#PC2GPUBenchmarks.StressTestStoreResults","page":"GPU Stress Test","title":"PC2GPUBenchmarks.StressTestStoreResults","text":"GPU stress test (matrix multiplications) in which we store all matmul results and try to run as many iterations as possible for a certain memory limit (default: 90% of free memory).\n\nThis stress test is somewhat inspired by gpu-burn by Ville Timonen.\n\n\n\n\n\n","category":"type"},{"location":"examples/gpuinfo/#Example:-CUDA/GPU-Information","page":"GPU Information","title":"Example: CUDA/GPU Information","text":"","category":"section"},{"location":"examples/gpuinfo/","page":"GPU Information","title":"GPU Information","text":"note: Note\nTest system: DGX with 8x A100 GPUs","category":"page"},{"location":"examples/gpuinfo/","page":"GPU Information","title":"GPU Information","text":"julia> gpuinfo()\nCUDA toolkit 11.5, local installation\nNVIDIA driver 495.29.5, for CUDA 11.5\nCUDA driver 11.5\n\nLibraries: \n- CUBLAS: 11.7.3\n- CURAND: 10.2.6\n- CUFFT: 10.6.0\n- CUSOLVER: 11.2.1\n- CUSPARSE: 11.7.0\n- CUPTI: 16.0.0\n- NVML: 11.0.0+495.29.5\n- CUDNN: missing\n- CUTENSOR: missing\n\nToolchain:\n- Julia: 1.7.1\n- LLVM: 12.0.1\n- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0\n- Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80\n\nEnvironment:\n- JULIA_CUDA_USE_BINARYBUILDER: false\n\n8 devices:\n  0: NVIDIA A100-SXM4-40GB (sm_80, 39.583 GiB / 39.586 GiB available)\n  1: NVIDIA A100-SXM4-40GB (sm_80, 39.583 GiB / 39.586 GiB available)\n  2: NVIDIA A100-SXM4-40GB (sm_80, 39.583 GiB / 39.586 GiB available)\n  3: NVIDIA A100-SXM4-40GB (sm_80, 39.583 GiB / 39.586 GiB available)\n  4: NVIDIA A100-SXM4-40GB (sm_80, 39.583 GiB / 39.586 GiB available)\n  5: NVIDIA A100-SXM4-40GB (sm_80, 39.583 GiB / 39.586 GiB available)\n  6: NVIDIA A100-SXM4-40GB (sm_80, 39.583 GiB / 39.586 GiB available)\n  7: NVIDIA A100-SXM4-40GB (sm_80, 39.583 GiB / 39.586 GiB available)","category":"page"},{"location":"examples/gpuinfo/","page":"GPU Information","title":"GPU Information","text":"More specific information for each gpu device can be obtained with gpuinfo(deviceid::Integer).","category":"page"},{"location":"examples/gpuinfo/","page":"GPU Information","title":"GPU Information","text":"julia> gpuinfo(0) # first GPU\nDevice: NVIDIA A100-SXM4-40GB (CuDevice(0))\nTotal amount of global memory: 42.5 GB\nNumber of CUDA cores: 6912\nNumber of multiprocessors: 108 (64 CUDA cores each)\nGPU max. clock rate: 1410 Mhz\nMemory clock rate: 1215 Mhz\nMemory bus width: 5120-bit\nL2 cache size: 41.9 MB\nMax. texture dimension sizes (1D): 131072\nMax. texture dimension sizes (2D): 131072, 65536\nMax. texture dimension sizes (3D): 16384, 16384, 16384\nMax. layered 1D texture size: 32768 (2048 layers)\nMax. layered 2D texture size: 32768, 32768 (2048 layers)\nTotal amount of constant memory: 65.5 kB\nTotal amount of shared memory per block: 49.2 kB\nTotal number of registers available per block: 65536\nWarp size: 32\nMax. number of threads per multiprocessor: 2048\nMax. number of threads per block: 1024\nMax. dimension size of a thread block (x,y,z): 1024, 1024, 64\nMax. dimension size of a grid size (x,y,z): 2147483647, 65535, 65535\nTexture alignment: 512.0 B\nMaximum memory pitch: 2.1 GB\nConcurrent copy and kernel execution: Yes with 3 copy engine(s)\nRun time limit on kernels: No\nIntegrated GPU sharing host memory: No\nSupport host page-locked memory mapping: Yes\nConcurrent kernel execution: Yes\nAlignment requirement for surfaces: Yes\nDevice has ECC support: Yes\nDevice supports Unified Addressing (UVA): Yes\nDevice supports managed memory: Yes\nDevice supports compute preemption: Yes\nSupports cooperative kernel launch: Yes\nSupports multi-device co-op kernel launch: Yes\nDevice PCI domain ID / bus ID / device ID: 0 / 7 / 0\nCompute mode: Default (multiple host threads can use ::cudaSetDevice() with device simultaneously)","category":"page"},{"location":"examples/gpuinfo/","page":"GPU Information","title":"GPU Information","text":"Peer-to-peer access information can be retrived via gpuinfo_p2p_access.","category":"page"},{"location":"examples/gpuinfo/","page":"GPU Information","title":"GPU Information","text":"julia> gpuinfo_p2p_access()\nP2P Access Supported:\n8×8 Matrix{Bool}:\n 0  1  1  1  1  1  1  1\n 1  0  1  1  1  1  1  1\n 1  1  0  1  1  1  1  1\n 1  1  1  0  1  1  1  1\n 1  1  1  1  0  1  1  1\n 1  1  1  1  1  0  1  1\n 1  1  1  1  1  1  0  1\n 1  1  1  1  1  1  1  0\n\nP2P Atomic Supported:\n8×8 Matrix{Bool}:\n 0  1  1  1  1  1  1  1\n 1  0  1  1  1  1  1  1\n 1  1  0  1  1  1  1  1\n 1  1  1  0  1  1  1  1\n 1  1  1  1  0  1  1  1\n 1  1  1  1  1  0  1  1\n 1  1  1  1  1  1  0  1\n 1  1  1  1  1  1  1  0","category":"page"},{"location":"examples/gpuinfo/","page":"GPU Information","title":"GPU Information","text":"Turns out that using CU_DEVICE_P2P_ATTRIBUTE_ACCESS_SUPPORTED or cuDeviceCanAccessPeer to query p2p access support may lead to different results (see this stackoverflow thread). In gpuinfo_p2p_access() we use both methods and, if the results were to be different, we print both matrices (not shown above).","category":"page"},{"location":"devices/v100_sxm2/#Tesla-V100-SXM2-32GB-(sm_70,-32.00-GiB)","page":"V100 SXM2","title":"Tesla V100-SXM2-32GB (sm_70, 32.00 GiB)","text":"","category":"section"},{"location":"devices/v100_sxm2/","page":"V100 SXM2","title":"V100 SXM2","text":"Benchmarks were run on the GPU cluster of the physics faculty of University of Bielefeld. ","category":"page"},{"location":"devices/v100_sxm2/","page":"V100 SXM2","title":"V100 SXM2","text":"For comparison: Datasheet by NVIDIA.","category":"page"},{"location":"devices/v100_sxm2/#Peakflops","page":"V100 SXM2","title":"Peakflops","text":"","category":"section"},{"location":"devices/v100_sxm2/#Theoretical","page":"V100 SXM2","title":"Theoretical","text":"","category":"section"},{"location":"devices/v100_sxm2/","page":"V100 SXM2","title":"V100 SXM2","text":"julia> theoretical_peakflops_gpu(; dtype=Float32);\nTheoretical Peakflops (TFLOP/s):\n └ max: 15.7\n\njulia> theoretical_peakflops_gpu(; dtype=Float64);\nTheoretical Peakflops (TFLOP/s):\n └ max: 7.8\n\n julia> theoretical_peakflops_gpu_tensorcores(; dtype=Float16);\nTheoretical Peakflops (TFLOP/s):\n └ max: 125.3","category":"page"},{"location":"devices/v100_sxm2/#Empirical","page":"V100 SXM2","title":"Empirical","text":"","category":"section"},{"location":"devices/v100_sxm2/","page":"V100 SXM2","title":"V100 SXM2","text":"julia> peakflops_gpu_fmas(; dtype=Float32);\nPeakflops (TFLOP/s):\n └ max: 15.47\n\njulia> peakflops_gpu_fmas(; dtype=Float64);\nPeakflops (TFLOP/s):\n └ max: 7.74\n\njulia> peakflops_gpu_wmmas(; dtype=Float16)\nPeakflops (TFLOP/s):\n └ max: 116.39","category":"page"},{"location":"devices/v100_sxm2/#Memory-bandwidth","page":"V100 SXM2","title":"Memory bandwidth","text":"","category":"section"},{"location":"devices/v100_sxm2/","page":"V100 SXM2","title":"V100 SXM2","text":"julia> memory_bandwidth();\nMemory Bandwidth (GiB/s):\n └ max: 719.75\n\njulia> GiB(719.75) |> change_base\n~772.83 GB\n\njulia> memory_bandwidth(; size=2^20*200) |> GiB |> change_base\nMemory Bandwidth (GiB/s):\n └ max: 755.14\n~810.82 GB","category":"page"},{"location":"devices/v100_sxm2/","page":"V100 SXM2","title":"V100 SXM2","text":"julia> host2device_bandwidth()\nMemsize: 512.000 MiB\nGPU: CuDevice(3) - Tesla V100-SXM2-32GB\n\nHost <-> Device Bandwidth (GiB/s):\n └ max: 4.46\n\nHost (pinned) <-> Device Bandwidth (GiB/s):\n └ max: 11.52\n\nDevice <-> Device (same device) Bandwidth (GiB/s):\n └ max: 724.45","category":"page"},{"location":"devices/v100_sxm2/#Peer-to-peer-bandwidth","page":"V100 SXM2","title":"Peer-to-peer bandwidth","text":"","category":"section"},{"location":"devices/v100_sxm2/","page":"V100 SXM2","title":"V100 SXM2","text":"julia> p2p_bandwidth();\nMemsize: 38.147 MiB\n\nBandwidth (GiB/s):\n ├ max: 22.36\n ├ min: 21.92\n ├ avg: 22.22\n └ std_dev: 0.18\n\njulia> p2p_bandwidth_all()\n8_8 Matrix{Union{Nothing, Float64}}:\n   nothing  22.5406    45.0071    22.5367    44.9879     8.11161    8.0912     8.09942\n 22.4844      nothing  22.5393    44.9706     8.11274   45.0036     8.08732    8.10026\n 44.7958    22.5406      nothing  45.0158     8.10381    8.11014   22.5406     7.39972\n 22.5301    45.0088    45.0001      nothing   7.428      8.10646    8.08204   22.5401\n 44.9897     7.14454    7.14674    7.14906     nothing  22.5371    44.9723    22.538\n  6.63411   44.9966     7.14542    7.14467   22.5371      nothing  22.5362    44.9949\n  7.75585    7.75844   22.5375     8.10314   44.9966    22.5353      nothing  44.9984\n  7.75575    7.75911    8.10116   22.5371    22.5367    44.9966    44.9914      nothing","category":"page"},{"location":"devices/v100_sxm2/#GPU-information","page":"V100 SXM2","title":"GPU information","text":"","category":"section"},{"location":"devices/v100_sxm2/","page":"V100 SXM2","title":"V100 SXM2","text":"julia> gpuinfo()\nCUDA toolkit 11.6, local installation\nNVIDIA driver 510.47.3, for CUDA 11.6\nCUDA driver 11.6\n\nLibraries: \n- CUBLAS: 11.8.1\n- CURAND: 10.2.9\n- CUFFT: 10.7.1\n- CUSOLVER: 11.3.3\n- CUSPARSE: 11.7.2\n- CUPTI: 16.0.0\n- NVML: 11.0.0+510.47.3\n- CUDNN: missing\n- CUTENSOR: missing\n\nToolchain:\n- Julia: 1.7.1\n- LLVM: 12.0.1\n- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0\n- Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80\n\nEnvironment:\n- JULIA_CUDA_USE_BINARYBUILDER: false\n\n8 devices:\n  0: Tesla V100-SXM2-32GB (sm_70, 31.449 GiB / 32.000 GiB available)\n  1: Tesla V100-SXM2-32GB (sm_70, 31.745 GiB / 32.000 GiB available)\n  2: Tesla V100-SXM2-32GB (sm_70, 31.745 GiB / 32.000 GiB available)\n  3: Tesla V100-SXM2-32GB (sm_70, 31.745 GiB / 32.000 GiB available)\n  4: Tesla V100-SXM2-32GB (sm_70, 31.745 GiB / 32.000 GiB available)\n  5: Tesla V100-SXM2-32GB (sm_70, 31.745 GiB / 32.000 GiB available)\n  6: Tesla V100-SXM2-32GB (sm_70, 31.745 GiB / 32.000 GiB available)\n  7: Tesla V100-SXM2-32GB (sm_70, 31.745 GiB / 32.000 GiB available)\n\njulia> gpuinfo(0)\nDevice: Tesla V100-SXM2-32GB (CuDevice(0))\nTotal amount of global memory: 31.749 GiB\nNumber of CUDA cores: 5120\nNumber of multiprocessors: 80 (64 CUDA cores each)\nGPU max. clock rate: 1530 MHz\nMemory clock rate: 877 MHz\nMemory bus width: 4096-bit\nL2 cache size: 6.000 MiB\nMax. texture dimension sizes (1D): 131072\nMax. texture dimension sizes (2D): 131072, 65536\nMax. texture dimension sizes (3D): 16384, 16384, 16384\nMax. layered 1D texture size: 32768 (2048 layers)\nMax. layered 2D texture size: 32768, 32768 (2048 layers)\nTotal amount of constant memory: 64.000 KiB\nTotal amount of shared memory per block: 48.000 KiB\nTotal number of registers available per block: 65536\nWarp size: 32\nMax. number of threads per multiprocessor: 2048\nMax. number of threads per block: 1024\nMax. dimension size of a thread block (x,y,z): 1024, 1024, 64\nMax. dimension size of a grid size (x,y,z): 2147483647, 65535, 65535\nTexture alignment: 512 bytes\nMaximum memory pitch: 2.000 GiB\nConcurrent copy and kernel execution: Yes with 6 copy engine(s)\nRun time limit on kernels: No\nIntegrated GPU sharing host memory: No\nSupport host page-locked memory mapping: Yes\nConcurrent kernel execution: Yes\nAlignment requirement for surfaces: Yes\nDevice has ECC support: Yes\nDevice supports Unified Addressing (UVA): Yes\nDevice supports managed memory: Yes\nDevice supports compute preemption: Yes\nSupports cooperative kernel launch: Yes\nSupports multi-device co-op kernel launch: Yes\nDevice PCI domain ID / bus ID / device ID: 0 / 31 / 0\nCompute mode: Default (multiple host threads can use ::cudaSetDevice() with device simultaneously)\n\njulia> gpuinfo_p2p_access()\nP2P Access Supported:\n8_8 Matrix{Bool}:\n 0  1  1  1  1  1  1  1\n 1  0  1  1  1  1  1  1\n 1  1  0  1  1  1  1  1\n 1  1  1  0  1  1  1  1\n 1  1  1  1  0  1  1  1\n 1  1  1  1  1  0  1  1\n 1  1  1  1  1  1  0  1\n 1  1  1  1  1  1  1  0\n\nP2P Atomic Supported:\n8_8 Matrix{Bool}:\n 0  1  1  1  1  0  0  0\n 1  0  1  1  0  1  0  0\n 1  1  0  1  0  0  1  0\n 1  1  1  0  0  0  0  1\n 1  0  0  0  0  1  1  1\n 0  1  0  0  1  0  1  1\n 0  0  1  0  1  1  0  1\n 0  0  0  1  1  1  1  0","category":"page"},{"location":"devices/a100_sxm2/#NVIDIA-A100-SXM4-40GB-(sm_80,-39.586-GiB)","page":"A100 SXM2","title":"NVIDIA A100-SXM4-40GB (sm_80, 39.586 GiB)","text":"","category":"section"},{"location":"devices/a100_sxm2/","page":"A100 SXM2","title":"A100 SXM2","text":"Benchmarks were run on a DGX-A100 at PC2. ","category":"page"},{"location":"devices/a100_sxm2/","page":"A100 SXM2","title":"A100 SXM2","text":"For comparison: Datasheet by NVIDIA.","category":"page"},{"location":"devices/a100_sxm2/#Peakflops","page":"A100 SXM2","title":"Peakflops","text":"","category":"section"},{"location":"devices/a100_sxm2/#CUDA-cores","page":"A100 SXM2","title":"CUDA cores","text":"","category":"section"},{"location":"devices/a100_sxm2/","page":"A100 SXM2","title":"A100 SXM2","text":"julia> theoretical_peakflops_gpu(; dtype=Float32);\nTheoretical Peakflops (TFLOP/s):\n └ max: 19.5\n\njulia> theoretical_peakflops_gpu(; dtype=Float64);\nTheoretical Peakflops (TFLOP/s):\n └ max: 9.7","category":"page"},{"location":"devices/a100_sxm2/","page":"A100 SXM2","title":"A100 SXM2","text":"julia> peakflops_gpu_fmas(; dtype=Float32);\nPeakflops (TFLOP/s):\n └ max: 19.14\n\njulia> peakflops_gpu_fmas(; dtype=Float64);\nPeakflops (TFLOP/s):\n └ max: 9.58\n\njulia> peakflops_gpu_fmas(; dtype=Float16);\nPeakflops (TFLOP/s):\n └ max: 12.77","category":"page"},{"location":"devices/a100_sxm2/#Tensor-cores","page":"A100 SXM2","title":"Tensor cores","text":"","category":"section"},{"location":"devices/a100_sxm2/","page":"A100 SXM2","title":"A100 SXM2","text":"julia> theoretical_peakflops_gpu_tensorcores(; dtype=Int8);\nTheoretical Peakflops (TOP/s):\n └ max: 623.7\n\njulia> theoretical_peakflops_gpu_tensorcores(; dtype=Float16);\nTheoretical Peakflops (TFLOP/s):\n └ max: 311.9\n\njulia> theoretical_peakflops_gpu_tensorcores(; dtype=Float32);\nTheoretical Peakflops (TFLOP/s):\n └ max: 155.9","category":"page"},{"location":"devices/a100_sxm2/","page":"A100 SXM2","title":"A100 SXM2","text":"julia> peakflops_gpu_wmmas(; dtype=Int8); # as of writing, only works with CUDA.jl PR 1119\nPeakflops (TOP/s):\n └ max: 620.11\n\njulia> peakflops_gpu_wmmas(; dtype=Float16);\nPeakflops (TFLOP/s):\n └ max: 311.16\n\njulia> peakflops_gpu_wmmas(; dtype=:TensorFloat32); # as of writing, only works with Julia >= 1.8.0 and CUDA.jl PR 1419\nPeakflops (TFLOP/s):\n └ max: 155.55","category":"page"},{"location":"devices/a100_sxm2/#Memory-bandwidth","page":"A100 SXM2","title":"Memory bandwidth","text":"","category":"section"},{"location":"devices/a100_sxm2/","page":"A100 SXM2","title":"A100 SXM2","text":"julia> memory_bandwidth();\nMemory Bandwidth (GiB/s):\n └ max: 1192.09\n\njulia> memory_bandwidth(; cublas=false); # use a custom saxpy kernel\nMemory Bandwidth (GiB/s):\n └ max: 1179.8\n\njulia> GiB(1192.09) |> change_base\n~1280.0 GB","category":"page"},{"location":"devices/a100_sxm2/","page":"A100 SXM2","title":"A100 SXM2","text":"julia> host2device_bandwidth()\nMemsize: 512.000 MiB\nGPU: CuDevice(0) - NVIDIA A100-SXM4-40GB\n\nHost <-> Device Bandwidth (GiB/s):\n └ max: 11.75\n\nHost (pinned) <-> Device Bandwidth (GiB/s):\n └ max: 24.24\n\nDevice <-> Device (same device) Bandwidth (GiB/s):\n └ max: 1226.84","category":"page"},{"location":"devices/a100_sxm2/#Peer-to-peer-bandwidth","page":"A100 SXM2","title":"Peer-to-peer bandwidth","text":"","category":"section"},{"location":"devices/a100_sxm2/","page":"A100 SXM2","title":"A100 SXM2","text":"julia> p2p_bandwidth();\nMemsize: 38.147 MiB\n\nBandwidth (GiB/s):\n ├ max: 247.32\n ├ min: 173.5\n ├ avg: 229.63\n └ std_dev: 31.67\n\njulia> p2p_bandwidth_all()\n8×8 Matrix{Union{Nothing, Float64}}:\n    nothing  245.706     241.075     244.467     246.434     242.229     245.085     245.033\n 239.046        nothing  241.776     243.853     241.626     245.136     244.467     240.379\n 246.957     242.633        nothing  242.937     245.291     248.114     239.193     242.684\n 244.724     241.375     244.211        nothing  245.861     238.117     245.085     242.28\n 241.576     246.329     242.582     245.602        nothing  246.59      240.677     243.343\n 247.114     240.18      245.965     244.006     236.616        nothing  242.28      244.673\n 243.802     242.028     248.326     239.933     244.365     245.033        nothing  245.498\n 245.136     246.904     239.488     243.343     244.057     240.627     243.445        nothing","category":"page"},{"location":"devices/a100_sxm2/#GPU-information","page":"A100 SXM2","title":"GPU information","text":"","category":"section"},{"location":"devices/a100_sxm2/","page":"A100 SXM2","title":"A100 SXM2","text":"julia> gpuinfo()\nCUDA toolkit 11.5, local installation\nNVIDIA driver 495.29.5, for CUDA 11.5\nCUDA driver 11.5\n\nLibraries: \n- CUBLAS: 11.7.3\n- CURAND: 10.2.6\n- CUFFT: 10.6.0\n- CUSOLVER: 11.2.1\n- CUSPARSE: 11.7.0\n- CUPTI: 16.0.0\n- NVML: 11.0.0+495.29.5\n- CUDNN: missing\n- CUTENSOR: missing\n\nToolchain:\n- Julia: 1.7.1\n- LLVM: 12.0.1\n- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0\n- Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80\n\nEnvironment:\n- JULIA_CUDA_USE_BINARYBUILDER: false\n\n8 devices:\n  0: NVIDIA A100-SXM4-40GB (sm_80, 39.583 GiB / 39.586 GiB available)\n  1: NVIDIA A100-SXM4-40GB (sm_80, 39.583 GiB / 39.586 GiB available)\n  2: NVIDIA A100-SXM4-40GB (sm_80, 39.583 GiB / 39.586 GiB available)\n  3: NVIDIA A100-SXM4-40GB (sm_80, 39.583 GiB / 39.586 GiB available)\n  4: NVIDIA A100-SXM4-40GB (sm_80, 39.583 GiB / 39.586 GiB available)\n  5: NVIDIA A100-SXM4-40GB (sm_80, 39.583 GiB / 39.586 GiB available)\n  6: NVIDIA A100-SXM4-40GB (sm_80, 39.583 GiB / 39.586 GiB available)\n  7: NVIDIA A100-SXM4-40GB (sm_80, 39.583 GiB / 39.586 GiB available)\n\njulia> gpuinfo(0) # first GPU\nDevice: NVIDIA A100-SXM4-40GB (CuDevice(0))\nTotal amount of global memory: 42.5 GB\nNumber of CUDA cores: 6912\nNumber of multiprocessors: 108 (64 CUDA cores each)\nGPU max. clock rate: 1410 Mhz\nMemory clock rate: 1215 Mhz\nMemory bus width: 5120-bit\nL2 cache size: 41.9 MB\nMax. texture dimension sizes (1D): 131072\nMax. texture dimension sizes (2D): 131072, 65536\nMax. texture dimension sizes (3D): 16384, 16384, 16384\nMax. layered 1D texture size: 32768 (2048 layers)\nMax. layered 2D texture size: 32768, 32768 (2048 layers)\nTotal amount of constant memory: 65.5 kB\nTotal amount of shared memory per block: 49.2 kB\nTotal number of registers available per block: 65536\nWarp size: 32\nMax. number of threads per multiprocessor: 2048\nMax. number of threads per block: 1024\nMax. dimension size of a thread block (x,y,z): 1024, 1024, 64\nMax. dimension size of a grid size (x,y,z): 2147483647, 65535, 65535\nTexture alignment: 512.0 B\nMaximum memory pitch: 2.1 GB\nConcurrent copy and kernel execution: Yes with 3 copy engine(s)\nRun time limit on kernels: No\nIntegrated GPU sharing host memory: No\nSupport host page-locked memory mapping: Yes\nConcurrent kernel execution: Yes\nAlignment requirement for surfaces: Yes\nDevice has ECC support: Yes\nDevice supports Unified Addressing (UVA): Yes\nDevice supports managed memory: Yes\nDevice supports compute preemption: Yes\nSupports cooperative kernel launch: Yes\nSupports multi-device co-op kernel launch: Yes\nDevice PCI domain ID / bus ID / device ID: 0 / 7 / 0\nCompute mode: Default (multiple host threads can use ::cudaSetDevice() with device simultaneously)\n\njulia> gpuinfo_p2p_access()\nP2P Access Supported:\n8×8 Matrix{Bool}:\n 0  1  1  1  1  1  1  1\n 1  0  1  1  1  1  1  1\n 1  1  0  1  1  1  1  1\n 1  1  1  0  1  1  1  1\n 1  1  1  1  0  1  1  1\n 1  1  1  1  1  0  1  1\n 1  1  1  1  1  1  0  1\n 1  1  1  1  1  1  1  0\n\nP2P Atomic Supported:\n8×8 Matrix{Bool}:\n 0  1  1  1  1  1  1  1\n 1  0  1  1  1  1  1  1\n 1  1  0  1  1  1  1  1\n 1  1  1  0  1  1  1  1\n 1  1  1  1  0  1  1  1\n 1  1  1  1  1  0  1  1\n 1  1  1  1  1  1  0  1\n 1  1  1  1  1  1  1  0","category":"page"},{"location":"refs/stresstest_cpu/#CPU-Stresstest","page":"CPU Stress Test","title":"CPU Stresstest","text":"","category":"section"},{"location":"refs/stresstest_cpu/#Index","page":"CPU Stress Test","title":"Index","text":"","category":"section"},{"location":"refs/stresstest_cpu/","page":"CPU Stress Test","title":"CPU Stress Test","text":"Pages   = [\"stresstest_cpu.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/stresstest_cpu/#References","page":"CPU Stress Test","title":"References","text":"","category":"section"},{"location":"refs/stresstest_cpu/","page":"CPU Stress Test","title":"CPU Stress Test","text":"Modules = [PC2GPUBenchmarks]\nPages   = [\"stresstest_cpu.jl\"]","category":"page"},{"location":"refs/stresstest_cpu/#PC2GPUBenchmarks.stresstest_cpu-Tuple{Any}","page":"CPU Stress Test","title":"PC2GPUBenchmarks.stresstest_cpu","text":"stresstest_cpu(core_or_cores)\n\nRun a CPU stress test (matrix multiplication) on one or multiple CPU cores, as specified by the positional argument. If no argument is provided (only) the currently active CPU core will be used.\n\nKeyword arguments:\n\nduration: stress test will take about the given time in seconds.\ndtype (default: Float64): element type of the matrices\nsize (default: floor(Int, sqrt(L2_cachesize() / sizeof(dtype)))): matrices of size (size, size) will be used\nverbose (default: true): toggle printing of information\nparallel (default: true): If true, will (try to) run each CPU core test on a different Julia thread. Make sure to have enough Julia threads.\nthreads (default: nothing): If parallel == true, this argument may be used to specify the Julia threads to use.\n\n\n\n\n\n","category":"method"},{"location":"refs/data_bandwidth/#Data-Bandwidth","page":"Data Bandwidth","title":"Data Bandwidth","text":"","category":"section"},{"location":"refs/data_bandwidth/#Index","page":"Data Bandwidth","title":"Index","text":"","category":"section"},{"location":"refs/data_bandwidth/","page":"Data Bandwidth","title":"Data Bandwidth","text":"Pages   = [\"data_bandwidth.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/data_bandwidth/#References","page":"Data Bandwidth","title":"References","text":"","category":"section"},{"location":"refs/data_bandwidth/","page":"Data Bandwidth","title":"Data Bandwidth","text":"Modules = [PC2GPUBenchmarks]\nPages   = [\"memory_bandwidth.jl\", \"memory_bandwidth_saxpy.jl\", \"host2device_bandwidth.jl\", \"p2p_bandwidth.jl\"]","category":"page"},{"location":"refs/data_bandwidth/#PC2GPUBenchmarks.memory_bandwidth_scaling-Tuple{}","page":"Data Bandwidth","title":"PC2GPUBenchmarks.memory_bandwidth_scaling","text":"memory_bandwidth_scaling() -> datasizes, bandwidths\n\nMeasures the memory bandwidth (via memory_bandwidth) as a function of data size. If verbose=true (default), displays a unicode plot. Returns the considered data sizes and GiB/s. For further options, see memory_bandwidth.\n\n\n\n\n\n","category":"method"},{"location":"refs/data_bandwidth/#PC2GPUBenchmarks.theoretical_memory_bandwidth","page":"Data Bandwidth","title":"PC2GPUBenchmarks.theoretical_memory_bandwidth","text":"theoretical_memory_bandwidth(; device::CuDevice=CUDA.device(); verbose=true)\n\nEstimates the theoretical maximal GPU memory bandwidth in GiB/s.\n\n\n\n\n\n","category":"function"},{"location":"refs/data_bandwidth/#PC2GPUBenchmarks.memory_bandwidth_saxpy-Tuple{}","page":"Data Bandwidth","title":"PC2GPUBenchmarks.memory_bandwidth_saxpy","text":"Tries to estimate the peak memory bandwidth of a GPU in GiB/s by measuring the time it takes to perform a SAXPY, i.e. a * x[i] + y[i].\n\nKeyword arguments:\n\ndevice (default: CUDA.device()): CUDA device to be used.\ndtype (default: Float32): element type of the vectors.\nsize (default: 2^20 * 10): length of the vectors.\nnbench (default: 5): number of measurements to be performed the best of which is used for the GiB/s computation.\nverbose (default: true): toggle printing.\ncublas (default: true): toggle between CUDA.axpy! and a custom saxpy_gpu_kernel!.\n\nSee also: memory_bandwidth_saxpy_scaling.\n\n\n\n\n\n","category":"method"},{"location":"refs/data_bandwidth/#PC2GPUBenchmarks.memory_bandwidth_saxpy_scaling-Tuple{}","page":"Data Bandwidth","title":"PC2GPUBenchmarks.memory_bandwidth_saxpy_scaling","text":"memory_bandwidth_saxpy_scaling() -> sizes, bandwidths\n\nMeasures the memory bandwidth (via memory_bandwidth_saxpy) as a function of vector length. If verbose=true (default), displays a unicode plot. Returns the considered lengths and GiB/s. For further options, see memory_bandwidth_saxpy.\n\n\n\n\n\n","category":"method"},{"location":"refs/data_bandwidth/#PC2GPUBenchmarks.p2p_bandwidth","page":"Data Bandwidth","title":"PC2GPUBenchmarks.p2p_bandwidth","text":"p2p_bandwidth([memsize::UnitPrefixedBytes]; kwargs...)\n\nPerforms a peer-to-peer memory copy benchmark (time measurement) and returns an inter-gpu memory bandwidth estimate (in GiB/s) derived from it.\n\nKeyword arguments:\n\nsrc (default: 0): source device\ndst (default: 1): destination device\nnbench (default: 5): number of time measurements (i.e. p2p memcopies)\nverbose (default: true): set to false to turn off any printing.\nhist (default: false): when true, a UnicodePlots-based histogram is printed.\ntimes (default: false): toggle printing of measured times.\nalternate (default: false): alternate src and dst, i.e. copy data back and forth.\ndtype (default: Float32): see alloc_mem.\n\nExamples:\n\np2p_bandwidth()\np2p_bandwidth(MiB(1024))\np2p_bandwidth(KiB(20_000); dtype=Int32)\n\n\n\n\n\n","category":"function"},{"location":"refs/data_bandwidth/#PC2GPUBenchmarks.p2p_bandwidth_all-Tuple","page":"Data Bandwidth","title":"PC2GPUBenchmarks.p2p_bandwidth_all","text":"p2p_bandwidth_all(args...; kwargs...)\n\nRun p2p_bandwidth for all combinations of devices. Returns a matrix with the p2p memory bandwidth estimates.\n\n\n\n\n\n","category":"method"},{"location":"examples/data_bandwidth/#Example:-Data-Bandwidth","page":"Data Bandwidth","title":"Example: Data Bandwidth","text":"","category":"section"},{"location":"examples/data_bandwidth/","page":"Data Bandwidth","title":"Data Bandwidth","text":"note: Note\nTest system: DGX with 8x A100 GPUs (+ NVSwitch)","category":"page"},{"location":"examples/data_bandwidth/#Memory-Bandwidth","page":"Data Bandwidth","title":"Memory Bandwidth","text":"","category":"section"},{"location":"examples/data_bandwidth/","page":"Data Bandwidth","title":"Data Bandwidth","text":"You can benchmark the memory bandwidth of a GPU, i.e. how much data can be loaded within a second, with the function memory_bandwidth. Under the hood, this function transfers a certain amount of data (memcpy), times the operation, and computes the resulting memory bandwidth.","category":"page"},{"location":"examples/data_bandwidth/","page":"Data Bandwidth","title":"Data Bandwidth","text":"julia> memory_bandwidth();\nMemory Bandwidth (GiB/s):\n └ max: 1219.18","category":"page"},{"location":"examples/data_bandwidth/","page":"Data Bandwidth","title":"Data Bandwidth","text":"Note that this is reasonably close to the expected theoretical maximal memory bandwidth of our A100 test device.","category":"page"},{"location":"examples/data_bandwidth/","page":"Data Bandwidth","title":"Data Bandwidth","text":"julia> theoretical_memory_bandwidth();\nTheoretical Maximal Memory Bandwidth (GiB/s):\n └ max: 1448.4","category":"page"},{"location":"examples/data_bandwidth/","page":"Data Bandwidth","title":"Data Bandwidth","text":"The scaling (as a function of data size) can be assessed with memory_bandwidth_scaling.","category":"page"},{"location":"examples/data_bandwidth/","page":"Data Bandwidth","title":"Data Bandwidth","text":"julia> memory_bandwidth_scaling()\n\n              ⠀⠀⠀⠀Peak: 1225.3 GiB/s (size = 1.0 GiB)⠀⠀⠀ \n              ┌────────────────────────────────────────┐ \n         2000 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠔⠒⠉⠉⢸│ \n   GiB/s      │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠒⠉⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡎⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡜⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡎⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠜⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n            0 │⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⣀⡠⠔⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              └────────────────────────────────────────┘ \n              ⠀2⁰⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2³⁰⠀ \n              ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀data size⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ","category":"page"},{"location":"examples/data_bandwidth/#SAXPY","page":"Data Bandwidth","title":"SAXPY","text":"","category":"section"},{"location":"examples/data_bandwidth/","page":"Data Bandwidth","title":"Data Bandwidth","text":"Note that we also provide similar functions that are based on a SAXPY streaming kernel.","category":"page"},{"location":"examples/data_bandwidth/","page":"Data Bandwidth","title":"Data Bandwidth","text":"julia> memory_bandwidth_saxpy();\nMemory Bandwidth (GiB/s):\n └ max: 1192.09\n\njulia> memory_bandwidth_saxpy_scaling()\n\n              ⠀⠀Peak: 1275.34 GiB/s (size = 300.0 MiB)⠀⠀ \n              ┌────────────────────────────────────────┐ \n         1280 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⠀⠀⠀⢀│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⠤⠤⠔⠒⠙⠊⠊⠉⢹│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⠤⠔⠒⠊⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠔⠒⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠔⠊⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠔⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠊⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n   GiB/s      │⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠊⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⡰⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⡜⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⢀⠎⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⡠⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⡰⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⢀⠜⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n         1190 │⠎⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              └────────────────────────────────────────┘ \n              ⠀2²³⸱³²¹⁹⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2²⁸⸱²²⁸⁸⠀ \n              ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀vector length⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ","category":"page"},{"location":"examples/data_bandwidth/#Host-to-Device-Bandwidth","page":"Data Bandwidth","title":"Host-to-Device Bandwidth","text":"","category":"section"},{"location":"examples/data_bandwidth/","page":"Data Bandwidth","title":"Data Bandwidth","text":"How much data can be transferred per second from the host memory to the GPU (and the other way around)?","category":"page"},{"location":"examples/data_bandwidth/","page":"Data Bandwidth","title":"Data Bandwidth","text":"julia> host2device_bandwidth()\nHost <-> Device Bandwidth (GiB/s):\n └ max: 11.79\n\nHost (pinned) <-> Device Bandwidth (GiB/s):\n └ max: 24.33","category":"page"},{"location":"examples/data_bandwidth/#Peer-to-Peer-Bandwidth","page":"Data Bandwidth","title":"Peer-to-Peer Bandwidth","text":"","category":"section"},{"location":"examples/data_bandwidth/#Unidirectional","page":"Data Bandwidth","title":"Unidirectional","text":"","category":"section"},{"location":"examples/data_bandwidth/","page":"Data Bandwidth","title":"Data Bandwidth","text":"julia> p2p_bandwidth();\nMemsize: 38.147 MiB\n\nBandwidth (GiB/s):\n ├ max: 247.32\n ├ min: 173.5\n ├ avg: 229.63\n └ std_dev: 31.67\n\njulia> p2p_bandwidth_all()\n8×8 Matrix{Union{Nothing, Float64}}:\n    nothing  245.706     241.075     244.467     246.434     242.229     245.085     245.033\n 239.046        nothing  241.776     243.853     241.626     245.136     244.467     240.379\n 246.957     242.633        nothing  242.937     245.291     248.114     239.193     242.684\n 244.724     241.375     244.211        nothing  245.861     238.117     245.085     242.28\n 241.576     246.329     242.582     245.602        nothing  246.59      240.677     243.343\n 247.114     240.18      245.965     244.006     236.616        nothing  242.28      244.673\n 243.802     242.028     248.326     239.933     244.365     245.033        nothing  245.498\n 245.136     246.904     239.488     243.343     244.057     240.627     243.445        nothing","category":"page"},{"location":"examples/data_bandwidth/","page":"Data Bandwidth","title":"Data Bandwidth","text":"According to NVIDIA, the theoretical maximal peer-to-peer bandwidth for our A100 with NVSwitch is 300GB/s ≈ 279GiB/s, which is in reasonable agreement with our findings.","category":"page"},{"location":"examples/data_bandwidth/#Bidirectional","page":"Data Bandwidth","title":"Bidirectional","text":"","category":"section"},{"location":"examples/data_bandwidth/","page":"Data Bandwidth","title":"Data Bandwidth","text":"julia> p2p_bandwidth_bidirectional();\nMemsize: 38.147 MiB\n\nBandwidth (GiB/s):\n ├ max: 450.36\n ├ min: 448.66\n ├ avg: 449.69\n └ std_dev: 0.49\n\njulia> p2p_bandwidth_bidirectional_all()\n8×8 Matrix{Union{Nothing, Float64}}:\n    nothing  456.631     453.133     454.946     453.67      453.953     455.06      454.662\n 453.67         nothing  454.01      450.329     455.345     453.02      454.691     455.203\n 453.981     451.53         nothing  452.344     453.868     454.747     452.232     454.208\n 453.557     451.979     449.883        nothing  454.01      455.288     450.189     454.691\n 452.429     454.293     445.094     454.151        nothing  453.472     451.474     453.981\n 454.89      454.066     453.84      453.84      451.194        nothing  453.274     451.53\n 453.925     453.02      454.293     456.459     451.839     451.951        nothing  455.032\n 454.208     416.936     454.265     435.947     452.035     437.836     451.895        nothing","category":"page"},{"location":"examples/p2p_bandwidth/#Example:-Peer-to-Peer-Memcpy","page":"Example: Peer-to-Peer Memcpy","title":"Example: Peer-to-Peer Memcpy","text":"","category":"section"},{"location":"examples/p2p_bandwidth/","page":"Example: Peer-to-Peer Memcpy","title":"Example: Peer-to-Peer Memcpy","text":"note: Note\nTest system: DGX with 8x A100 GPUs (+ NVSwitch)","category":"page"},{"location":"examples/p2p_bandwidth/#Unidirectional","page":"Example: Peer-to-Peer Memcpy","title":"Unidirectional","text":"","category":"section"},{"location":"examples/p2p_bandwidth/","page":"Example: Peer-to-Peer Memcpy","title":"Example: Peer-to-Peer Memcpy","text":"julia> p2p_bandwidth();\nMemsize: 38.147 MiB\n\nBandwidth (GiB/s):\n ├ max: 247.32\n ├ min: 173.5\n ├ avg: 229.63\n └ std_dev: 31.67\n\njulia> p2p_bandwidth_all()\n8×8 Matrix{Union{Nothing, Float64}}:\n    nothing  245.706     241.075     244.467     246.434     242.229     245.085     245.033\n 239.046        nothing  241.776     243.853     241.626     245.136     244.467     240.379\n 246.957     242.633        nothing  242.937     245.291     248.114     239.193     242.684\n 244.724     241.375     244.211        nothing  245.861     238.117     245.085     242.28\n 241.576     246.329     242.582     245.602        nothing  246.59      240.677     243.343\n 247.114     240.18      245.965     244.006     236.616        nothing  242.28      244.673\n 243.802     242.028     248.326     239.933     244.365     245.033        nothing  245.498\n 245.136     246.904     239.488     243.343     244.057     240.627     243.445        nothing","category":"page"},{"location":"examples/p2p_bandwidth/","page":"Example: Peer-to-Peer Memcpy","title":"Example: Peer-to-Peer Memcpy","text":"Theoretical maximum should be 25GB/s * 12 = 300GB/s ≈ 279GiB/s.","category":"page"},{"location":"examples/p2p_bandwidth/#NVIDIA-Nsight-Systems-profile","page":"Example: Peer-to-Peer Memcpy","title":"NVIDIA Nsight Systems profile","text":"","category":"section"},{"location":"examples/p2p_bandwidth/","page":"Example: Peer-to-Peer Memcpy","title":"Example: Peer-to-Peer Memcpy","text":"(Image: p2p_memcpy_dgx_profile)","category":"page"},{"location":"examples/p2p_bandwidth/#Discussions","page":"Example: Peer-to-Peer Memcpy","title":"Discussions","text":"","category":"section"},{"location":"examples/p2p_bandwidth/","page":"Example: Peer-to-Peer Memcpy","title":"Example: Peer-to-Peer Memcpy","text":"See this GitHub gist.","category":"page"},{"location":"examples/p2p_bandwidth/#Bidirectional","page":"Example: Peer-to-Peer Memcpy","title":"Bidirectional","text":"","category":"section"},{"location":"examples/p2p_bandwidth/","page":"Example: Peer-to-Peer Memcpy","title":"Example: Peer-to-Peer Memcpy","text":"julia> p2p_bandwidth_bidirectional();\nMemsize: 38.147 MiB\n\nBandwidth (GiB/s):\n ├ max: 450.36\n ├ min: 448.66\n ├ avg: 449.69\n └ std_dev: 0.49","category":"page"},{"location":"examples/p2p_bandwidth/","page":"Example: Peer-to-Peer Memcpy","title":"Example: Peer-to-Peer Memcpy","text":"julia> p2p_bandwidth_bidirectional_all()\n8×8 Matrix{Union{Nothing, Float64}}:\n    nothing  456.631     453.133     454.946     453.67      453.953     455.06      454.662\n 453.67         nothing  454.01      450.329     455.345     453.02      454.691     455.203\n 453.981     451.53         nothing  452.344     453.868     454.747     452.232     454.208\n 453.557     451.979     449.883        nothing  454.01      455.288     450.189     454.691\n 452.429     454.293     445.094     454.151        nothing  453.472     451.474     453.981\n 454.89      454.066     453.84      453.84      451.194        nothing  453.274     451.53\n 453.925     453.02      454.293     456.459     451.839     451.951        nothing  455.032\n 454.208     416.936     454.265     435.947     452.035     437.836     451.895        nothing","category":"page"},{"location":"examples/p2p_bandwidth/","page":"Example: Peer-to-Peer Memcpy","title":"Example: Peer-to-Peer Memcpy","text":"Theoretical maximum should be 25GB/s * 12 * 2 = 600GB/s ≈ 559 GiB/s (is this correct?!).","category":"page"},{"location":"examples/p2p_bandwidth/#NVIDIA-Nsight-Systems-profile-2","page":"Example: Peer-to-Peer Memcpy","title":"NVIDIA Nsight Systems profile","text":"","category":"section"},{"location":"examples/p2p_bandwidth/","page":"Example: Peer-to-Peer Memcpy","title":"Example: Peer-to-Peer Memcpy","text":"(Image: p2p_memcpy_bidirectional_dgx_profile.png)","category":"page"},{"location":"refs/monitoring/#GPU-Monitoring","page":"GPU Monitoring","title":"GPU Monitoring","text":"","category":"section"},{"location":"refs/monitoring/#Index","page":"GPU Monitoring","title":"Index","text":"","category":"section"},{"location":"refs/monitoring/","page":"GPU Monitoring","title":"GPU Monitoring","text":"Pages   = [\"monitoring.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/monitoring/#References","page":"GPU Monitoring","title":"References","text":"","category":"section"},{"location":"refs/monitoring/","page":"GPU Monitoring","title":"GPU Monitoring","text":"Modules = [PC2GPUBenchmarks]\nPages   = [\"monitoring.jl\"]","category":"page"},{"location":"refs/monitoring/#PC2GPUBenchmarks.ismonitoring-Tuple{}","page":"GPU Monitoring","title":"PC2GPUBenchmarks.ismonitoring","text":"Checks if we are currently monitoring.\n\n\n\n\n\n","category":"method"},{"location":"refs/monitoring/#PC2GPUBenchmarks.livemonitor_powerusage-Tuple{Any}","page":"GPU Monitoring","title":"PC2GPUBenchmarks.livemonitor_powerusage","text":"livemonitor_powerusage(duration) -> times, powerusage\n\nMonitor the power usage of GPU(s) (in Watts) over a given time period, as specified by duration (in seconds). Returns the (relative) times as a Vector{Float64} and the power usage as a Vector{Vector{Float64}}.\n\nFor general keyword arguments, see livemonitor_something.\n\n\n\n\n\n","category":"method"},{"location":"refs/monitoring/#PC2GPUBenchmarks.livemonitor_something-Union{Tuple{F}, Tuple{F, Any}} where F","page":"GPU Monitoring","title":"PC2GPUBenchmarks.livemonitor_something","text":"livemonitor_something(f, duration) -> times, values\n\nMonitor some property of GPU(s), as specified through the function f, over a given time period, as specified by duration (in seconds). Returns the (relative) times as a Vector{Float64} and the temperatures as a Vector{Vector{Float64}}.\n\nThe function f will be called on a vector of devices (CuDevices or NVML.Devices) and should return a vector of Float64 values.\n\nKeyword arguments:\n\nfreq (default: 1): polling rate in Hz.\ndevices (default: NVML.devices()): CuDevices or NVML.Devices to consider.\nplot (default: false): Create a unicode plot after the monitoring.\nliveplot (default: false): Create and update a unicode plot during the monitoring. Use optional ylims to specify fixed y limits.\ntitle (default: \"\"): Title used in unicode plots.\nylabel (default: \"Values\"): y label used in unicode plots.\n\nSee: livemonitor_temperature, livemonitor_powerusage\n\n\n\n\n\n","category":"method"},{"location":"refs/monitoring/#PC2GPUBenchmarks.livemonitor_temperature-Tuple{Any}","page":"GPU Monitoring","title":"PC2GPUBenchmarks.livemonitor_temperature","text":"livemonitor_temperature(duration) -> times, temperatures\n\nMonitor the temperature of GPU(s) over a given time period, as specified by duration (in seconds). Returns the (relative) times as a Vector{Float64} and the temperatures as a Vector{Vector{Float64}}.\n\nFor general keyword arguments, see livemonitor_something.\n\n\n\n\n\n","category":"method"},{"location":"refs/monitoring/#PC2GPUBenchmarks.monitoring_start-Tuple{}","page":"GPU Monitoring","title":"PC2GPUBenchmarks.monitoring_start","text":"monitoring_start(; devices=CUDA.devices(), verbose=true)\n\nStart monitoring of GPU temperature, utilization, power usage, etc.\n\nKeyword arguments:\n\nfreq (default: 1): polling rate in Hz.\ndevices (default: CUDA.devices()): CuDevices or NVML.Devices to monitor.\nthread (default: Threads.nthreads()): id of the Julia thread that should run the monitoring.\nverbose (default: true): toggle verbose output.\n\nSee also monitoring_stop.\n\n\n\n\n\n","category":"method"},{"location":"refs/monitoring/#PC2GPUBenchmarks.monitoring_stop-Tuple{}","page":"GPU Monitoring","title":"PC2GPUBenchmarks.monitoring_stop","text":"monitoring_stop(; verbose=true) -> results\n\nStops the GPU monitoring and returns the measured values. Specifically, results is a named tuple with the following keys:\n\ntime: the (relative) times at which we measured\ntemperature, power, compute, mem\n\nSee also monitoring_start and plot_monitoring_results.\n\n\n\n\n\n","category":"method"},{"location":"refs/monitoring/#PC2GPUBenchmarks.plot_monitoring_results","page":"GPU Monitoring","title":"PC2GPUBenchmarks.plot_monitoring_results","text":"plot_monitoring_results(r::MonitoringResults, symbols=keys(r.results); tofile=false, ext=:pdf)\n\nPlot the quantities specified through symbols of a MonitoringResults object. If tofile==false (default), UnicodePlots.jl will be used to generate textual in-terminal / in-logfile plots. If tofile==true, CairoMakie.jl will be used to generate and save high-quality plots to disk.\n\n\n\n\n\n","category":"function"},{"location":"refs/peakflops_gpu/#Peakflops","page":"Peakflops","title":"Peakflops","text":"","category":"section"},{"location":"refs/peakflops_gpu/#Index","page":"Peakflops","title":"Index","text":"","category":"section"},{"location":"refs/peakflops_gpu/","page":"Peakflops","title":"Peakflops","text":"Pages   = [\"peakflops_gpu.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/peakflops_gpu/#References","page":"Peakflops","title":"References","text":"","category":"section"},{"location":"refs/peakflops_gpu/","page":"Peakflops","title":"Peakflops","text":"Modules = [PC2GPUBenchmarks]\nPages   = [\"peakflops_gpu.jl\", \"peakflops_gpu_matmul.jl\", \"peakflops_gpu_fmas.jl\", \"peakflops_gpu_wmmas.jl\"]","category":"page"},{"location":"refs/peakflops_gpu/#PC2GPUBenchmarks.peakflops_gpu-Tuple{}","page":"Peakflops","title":"PC2GPUBenchmarks.peakflops_gpu","text":"peakflops_gpu(; tensorcores=hastensorcores(), kwargs...)\n\nTries to estimate the peak performance of a GPU in TFLOP/s by measuring the time it takes to perform\n\n_kernel_fma_nfmas() * size many FMAs on CUDA cores (if tensorcores == false)\n_kernel_wmma_nwmmas() many WMMAs on Tensor Cores (if tensorcores == true)\n\nFor more keyword argument options see peakflops_gpu_fmas and peakflops_gpu_wmmas.\n\n\n\n\n\n","category":"method"},{"location":"refs/peakflops_gpu/#PC2GPUBenchmarks.theoretical_peakflops_gpu-Tuple{}","page":"Peakflops","title":"PC2GPUBenchmarks.theoretical_peakflops_gpu","text":"Estimates the theoretical peak performance of a CUDA device in TFLOP/s.\n\nKeyword arguments:\n\ntensorcores (default: hastensorcores()): toggle usage of tensore cores. If false, cuda cores will be used.\nverbose (default: true): toggle printing of information\ndevice (default: device()): CUDA device to be analyzed\ndtype (default: tensorcores ? Float16 : Float32): element type of the matrices\n\n\n\n\n\n","category":"method"},{"location":"refs/peakflops_gpu/#PC2GPUBenchmarks.peakflops_gpu_matmul-Tuple{}","page":"Peakflops","title":"PC2GPUBenchmarks.peakflops_gpu_matmul","text":"peakflops_gpu_matmul(; device, dtype=Float32, size=2^14, nmatmuls=5, nbench=5, verbose=true)\n\nTries to estimate the peak performance of a GPU in TFLOP/s by measuring the time it takes to perform nmatmuls many (in-place) matrix-matrix multiplications.\n\nKeyword arguments:\n\ndevice (default: CUDA.device()): CUDA device to be used.\ndtype (default: Float32): element type of the matrices.\nsize (default: 2^14): matrices will have dimensions (size, size).\nnmatmuls (default: 5): number of matmuls that will make up the kernel to be timed.\nnbench (default: 5): number of measurements to be performed the best of which is used for the TFLOP/s computation.\nverbose (default: true): toggle printing.\n\nSee also: peakflops_gpu_matmul_scaling, peakflops_gpu_matmul_graphs.\n\n\n\n\n\n","category":"method"},{"location":"refs/peakflops_gpu/#PC2GPUBenchmarks.peakflops_gpu_matmul_graphs-Tuple{}","page":"Peakflops","title":"PC2GPUBenchmarks.peakflops_gpu_matmul_graphs","text":"Same as peakflops_gpu_matmul but uses CUDA's graph API to define and launch the kernel.\n\nSee also: peakflops_gpu_matmul_scaling.\n\n\n\n\n\n","category":"method"},{"location":"refs/peakflops_gpu/#PC2GPUBenchmarks.peakflops_gpu_matmul_scaling-Union{Tuple{}, Tuple{F}} where F","page":"Peakflops","title":"PC2GPUBenchmarks.peakflops_gpu_matmul_scaling","text":"peakflops_gpu_matmul_scaling(peakflops_func = peakflops_gpu_matmul; verbose=true) -> sizes, flops\n\nAsserts the scaling of the given peakflops_function (defaults to peakflops_gpu_matmul) with increasing matrix size. If verbose=true (default), displays a unicode plot. Returns the considered sizes and TFLOP/s. For further options, see peakflops_gpu_matmul.\n\n\n\n\n\n","category":"method"},{"location":"refs/peakflops_gpu/#PC2GPUBenchmarks.kernel_fma-NTuple{4, Any}","page":"Peakflops","title":"PC2GPUBenchmarks.kernel_fma","text":"Dummy kernel doing _kernel_fma_nfmas() many FMAs (default: 100_000).\n\n\n\n\n\n","category":"method"},{"location":"refs/peakflops_gpu/#PC2GPUBenchmarks.peakflops_gpu_fmas-Tuple{}","page":"Peakflops","title":"PC2GPUBenchmarks.peakflops_gpu_fmas","text":"peakflops_gpu_fmas(; size::Integer=5_000_000, dtype=Float32, nbench=5, nkernel=5, device=CUDA.device(), verbose=true)\n\nTries to estimate the peak performance of a GPU in TFLOP/s by measuring the time it takes to perform _kernel_fma_nfmas() * size many FMAs on CUDA cores.\n\nKeyword arguments:\n\ndevice (default: CUDA.device()): CUDA device to be used.\ndtype (default: Float32): element type of the matrices.\nsize (default: 5_000_000): length of vectors.\nnkernel (default: 5): number of kernel calls that make up one benchmarking sample.\nnbench (default: 5): number of measurements to be performed the best of which is used for the TFLOP/s computation.\nverbose (default: true): toggle printing.\n\n\n\n\n\n","category":"method"},{"location":"refs/peakflops_gpu/#PC2GPUBenchmarks.peakflops_gpu_wmmas-Tuple{}","page":"Peakflops","title":"PC2GPUBenchmarks.peakflops_gpu_wmmas","text":"peakflops_gpu_wmmas()\n\nTries to estimate the peak performance of a GPU in TFLOP/s by measuring the time it takes to perform _kernel_wmma_nwmmas() many WMMAs on Tensor Cores.\n\nKeyword arguments:\n\ndevice (default: CUDA.device()): CUDA device to be used.\ndtype (default: Float16): element type of the matrices. We currently support Float16, Int8, and :TensorFloat32.\nnkernel (default: 10): number of kernel calls that make up one benchmarking sample.\nnbench (default: 5): number of measurements to be performed the best of which is used for the TFLOP/s computation.\nthreads (default: max. threads per block): how many threads to use per block (part of the kernel launch configuration).\nblocks (default: 2048): how many blocks to use (part of the kernel launch configuration).\nverbose (default: true): toggle printing.\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#Utility","page":"Utility","title":"Utility","text":"","category":"section"},{"location":"refs/utility/#Index","page":"Utility","title":"Index","text":"","category":"section"},{"location":"refs/utility/","page":"Utility","title":"Utility","text":"Pages   = [\"utility.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/utility/#References","page":"Utility","title":"References","text":"","category":"section"},{"location":"refs/utility/","page":"Utility","title":"Utility","text":"Modules = [PC2GPUBenchmarks]\nPages   = [\"utility.jl\", \"utility_unroll.jl\"]","category":"page"},{"location":"refs/utility/#PC2GPUBenchmarks.alloc_mem-Tuple{UnitPrefixedBytes}","page":"Utility","title":"PC2GPUBenchmarks.alloc_mem","text":"alloc_mem(memsize::UnitPrefixedBytes; devs=(CUDA.device(),), dtype=Float32)\n\nAllocates memory on the devices whose IDs are provided via devs. Returns a vector of memory handles (i.e. CuArrays).\n\nExamples:\n\nalloc_mem(MiB(1024)) # allocate on the currently active device\nalloc_mem(B(40_000_000); devs=(0,1)) # allocate on GPU0 and GPU1\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#PC2GPUBenchmarks.clear_all_gpus_memory","page":"Utility","title":"PC2GPUBenchmarks.clear_all_gpus_memory","text":"Reclaim the unused memory of all available GPUs.\n\n\n\n\n\n","category":"function"},{"location":"refs/utility/#PC2GPUBenchmarks.clear_gpu_memory","page":"Utility","title":"PC2GPUBenchmarks.clear_gpu_memory","text":"Reclaim the unused memory of the currently active GPU (i.e. device()).\n\n\n\n\n\n","category":"function"},{"location":"refs/utility/#PC2GPUBenchmarks.functional","page":"Utility","title":"PC2GPUBenchmarks.functional","text":"Check if CUDA/GPU is available and functional. If not, print some (hopefully useful) debug information.\n\n\n\n\n\n","category":"function"},{"location":"refs/utility/#PC2GPUBenchmarks.get_cpu_stats-Tuple{}","page":"Utility","title":"PC2GPUBenchmarks.get_cpu_stats","text":"Get information about all cpu cores. Returns a vector of vectors. The outer index corresponds to cpu cores. The inner vector contains the following information (in that order):\n\nuser nice system idle iowait irq softirq steal guest ?\n\nSee proc(5) for more information.\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#PC2GPUBenchmarks.get_cpu_utilization","page":"Utility","title":"PC2GPUBenchmarks.get_cpu_utilization","text":"get_cpu_utilization(core=getcpuid(); Δt=0.01)\n\nGet the utilization (in percent) of the given cpu core over a certain time interval Δt.\n\n\n\n\n\n","category":"function"},{"location":"refs/utility/#PC2GPUBenchmarks.get_cpu_utilizations","page":"Utility","title":"PC2GPUBenchmarks.get_cpu_utilizations","text":"get_cpu_utilizations(cores=0:Sys.CPU_THREADS-1; Δt=0.01)\n\nGet the utilization (in percent) of the given cpu cores over a certain time interval Δt.\n\nBased on this.\n\n\n\n\n\n","category":"function"},{"location":"refs/utility/#PC2GPUBenchmarks.get_cpusocket_temperatures-Tuple{}","page":"Utility","title":"PC2GPUBenchmarks.get_cpusocket_temperatures","text":"Tries to get the temperatures of the available CPUs (sockets not cores) in degrees Celsius.\n\nBased on cat /sys/class/thermal/thermal_zone*/temp.\n\n\n\n\n\n","category":"method"},{"location":"refs/utility/#PC2GPUBenchmarks.hastensorcores","page":"Utility","title":"PC2GPUBenchmarks.hastensorcores","text":"Checks whether the given CuDevice has Tensor Cores.\n\n\n\n\n\n","category":"function"},{"location":"refs/utility/#PC2GPUBenchmarks.toggle_tensorcoremath","page":"Utility","title":"PC2GPUBenchmarks.toggle_tensorcoremath","text":"toggle_tensorcoremath([enable::Bool; verbose=true])\n\nSwitches the CUDA.math_mode between CUDA.FAST_MATH (enable=true) and CUDA.DEFAULT_MATH (enable=false). For matmuls of CuArray{Float32}s, this should have the effect of using/enabling and not using/disabling tensor cores. Of course, this only works on supported devices and CUDA versions.\n\nIf no arguments are provided, this functions toggles between the two math modes.\n\n\n\n\n\n","category":"function"},{"location":"refs/utility/#PC2GPUBenchmarks.@unroll-Tuple{Any, Any}","page":"Utility","title":"PC2GPUBenchmarks.@unroll","text":"@unroll N expr\n\nTakes a for loop as expr and informs the LLVM unroller to unroll it N times, if it is safe to do so.\n\n\n\n\n\n","category":"macro"},{"location":"refs/utility/#PC2GPUBenchmarks.@unroll-Tuple{Any}","page":"Utility","title":"PC2GPUBenchmarks.@unroll","text":"@unroll expr Takes a for loop as expr and informs the LLVM unroller to fully unroll it, if it is safe to do so and the loop count is known.\n\n\n\n\n\n","category":"macro"},{"location":"devices/geforce_gtx_1650/#NVIDIA-GeForce-GTX-1650-(sm_75,-3.787-GiB)","page":"GeForce GTX 1650","title":"NVIDIA GeForce GTX 1650 (sm_75, 3.787 GiB)","text":"","category":"section"},{"location":"devices/geforce_gtx_1650/","page":"GeForce GTX 1650","title":"GeForce GTX 1650","text":"Some information by NVIDIA is available here. Some external numbers may be found here.","category":"page"},{"location":"devices/geforce_gtx_1650/#Peakflops","page":"GeForce GTX 1650","title":"Peakflops","text":"","category":"section"},{"location":"devices/geforce_gtx_1650/#Theoretical","page":"GeForce GTX 1650","title":"Theoretical","text":"","category":"section"},{"location":"devices/geforce_gtx_1650/","page":"GeForce GTX 1650","title":"GeForce GTX 1650","text":"julia> theoretical_peakflops_gpu(; dtype=Float32);\nTheoretical Peakflops (TFLOP/s):\n └ max: 2.9\n\njulia> theoretical_peakflops_gpu(; dtype=Float64);\nTheoretical Peakflops (TFLOP/s):\n └ max: 1.5","category":"page"},{"location":"devices/geforce_gtx_1650/#Empirical","page":"GeForce GTX 1650","title":"Empirical","text":"","category":"section"},{"location":"devices/geforce_gtx_1650/","page":"GeForce GTX 1650","title":"GeForce GTX 1650","text":"julia> peakflops_gpu_fmas(; dtype=Float32); # too large?\nPeakflops (TFLOP/s):\n └ max: 3.41\n\njulia> peakflops_gpu_fmas(; dtype=Float64); # too low?\nPeakflops (TFLOP/s):\n └ max: 0.11","category":"page"},{"location":"devices/geforce_gtx_1650/#Memory-bandwidth","page":"GeForce GTX 1650","title":"Memory bandwidth","text":"","category":"section"},{"location":"devices/geforce_gtx_1650/","page":"GeForce GTX 1650","title":"GeForce GTX 1650","text":"julia> memory_bandwidth();\nMemory Bandwidth (GiB/s):\n └ max: 159.33\n\njulia> GiB(159.33) |> change_base\n~171.08 GB","category":"page"},{"location":"devices/geforce_gtx_1650/","page":"GeForce GTX 1650","title":"GeForce GTX 1650","text":"julia> host2device_bandwidth()\nMemsize: 512.000 MiB\nGPU: CuDevice(0) - NVIDIA GeForce GTX 1650\n\nHost <-> Device Bandwidth (GiB/s):\n └ max: 12.06\n\nHost (pinned) <-> Device Bandwidth (GiB/s):\n └ max: 12.05\n\nDevice <-> Device (same device) Bandwidth (GiB/s):\n └ max: 153.0","category":"page"},{"location":"devices/geforce_gtx_1650/#GPU-information","page":"GeForce GTX 1650","title":"GPU information","text":"","category":"section"},{"location":"devices/geforce_gtx_1650/","page":"GeForce GTX 1650","title":"GeForce GTX 1650","text":"julia> gpuinfo()\nCUDA toolkit 11.6, artifact installation\nNVIDIA driver 470.86.0, for CUDA 11.4\nCUDA driver 11.4\n\nLibraries: \n- CUBLAS: 11.8.1\n- CURAND: 10.2.9\n- CUFFT: 10.7.0\n- CUSOLVER: 11.3.2\n- CUSPARSE: 11.7.1\n- CUPTI: 16.0.0\n- NVML: 11.0.0+470.86\n- CUDNN: 8.30.2 (for CUDA 11.5.0)\n- CUTENSOR: 1.4.0 (for CUDA 11.5.0)\n\nToolchain:\n- Julia: 1.7.2\n- LLVM: 12.0.1\n- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0\n- Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80\n\n1 device:\n  0: NVIDIA GeForce GTX 1650 (sm_75, 2.240 GiB / 3.787 GiB available)\n\njulia> gpuinfo(0)\nDevice: NVIDIA GeForce GTX 1650 (CuDevice(0))\nTotal amount of global memory: 3.787 GiB\nNumber of CUDA cores: 896\nNumber of multiprocessors: 14 (64 CUDA cores each)\nGPU max. clock rate: 1620 MHz\nMemory clock rate: 6001 MHz\nMemory bus width: 128-bit\nL2 cache size: 1024.000 KiB\nMax. texture dimension sizes (1D): 131072\nMax. texture dimension sizes (2D): 131072, 65536\nMax. texture dimension sizes (3D): 16384, 16384, 16384\nMax. layered 1D texture size: 32768 (2048 layers)\nMax. layered 2D texture size: 32768, 32768 (2048 layers)\nTotal amount of constant memory: 64.000 KiB\nTotal amount of shared memory per block: 48.000 KiB\nTotal number of registers available per block: 65536\nWarp size: 32\nMax. number of threads per multiprocessor: 1024\nMax. number of threads per block: 1024 \nMax. dimension size of a thread block (x,y,z): 1024, 1024, 64\nMax. dimension size of a grid size (x,y,z): 2147483647, 65535, 65535\nTexture alignment: 512 bytes\nMaximum memory pitch: 2.000 GiB\nConcurrent copy and kernel execution: Yes with 3 copy engine(s)\nRun time limit on kernels: Yes\nIntegrated GPU sharing host memory: No \nSupport host page-locked memory mapping: Yes\nConcurrent kernel execution: Yes\nAlignment requirement for surfaces: Yes\nDevice has ECC support: No\nDevice supports Unified Addressing (UVA): Yes\nDevice supports managed memory: Yes\nDevice supports compute preemption: Yes\nSupports cooperative kernel launch: Yes\nSupports multi-device co-op kernel launch: Yes\nDevice PCI domain ID / bus ID / device ID: 0 / 1 / 0\nCompute mode: Default (multiple host threads can use ::cudaSetDevice() with device simultaneously)","category":"page"},{"location":"refs/UnitPrefixedBytes/#Unit-Prefixed-Bytes","page":"Unit-Prefixed Bytes","title":"Unit-Prefixed Bytes","text":"","category":"section"},{"location":"refs/UnitPrefixedBytes/#Index","page":"Unit-Prefixed Bytes","title":"Index","text":"","category":"section"},{"location":"refs/UnitPrefixedBytes/","page":"Unit-Prefixed Bytes","title":"Unit-Prefixed Bytes","text":"Pages   = [\"UnitPrefixedBytes.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/UnitPrefixedBytes/#References","page":"Unit-Prefixed Bytes","title":"References","text":"","category":"section"},{"location":"refs/UnitPrefixedBytes/","page":"Unit-Prefixed Bytes","title":"Unit-Prefixed Bytes","text":"Modules = [PC2GPUBenchmarks]\nPages   = [\"UnitPrefixedBytes.jl\"]","category":"page"},{"location":"refs/UnitPrefixedBytes/#PC2GPUBenchmarks.B","page":"Unit-Prefixed Bytes","title":"PC2GPUBenchmarks.B","text":"Bytes\n\n\n\n\n\n","category":"type"},{"location":"refs/UnitPrefixedBytes/#PC2GPUBenchmarks.GB","page":"Unit-Prefixed Bytes","title":"PC2GPUBenchmarks.GB","text":"Gigabytes, i.e. 10^9 = 1000^3 bytes\n\n\n\n\n\n","category":"type"},{"location":"refs/UnitPrefixedBytes/#PC2GPUBenchmarks.GiB","page":"Unit-Prefixed Bytes","title":"PC2GPUBenchmarks.GiB","text":"Gibibytes, i.e. 2^30 = 1024^3 bytes\n\n\n\n\n\n","category":"type"},{"location":"refs/UnitPrefixedBytes/#PC2GPUBenchmarks.KB","page":"Unit-Prefixed Bytes","title":"PC2GPUBenchmarks.KB","text":"Kilobytes, i.e. 10^3 = 1000 bytes\n\n\n\n\n\n","category":"type"},{"location":"refs/UnitPrefixedBytes/#PC2GPUBenchmarks.KiB","page":"Unit-Prefixed Bytes","title":"PC2GPUBenchmarks.KiB","text":"Kibibytes, i.e. 2^10 = 1024 bytes\n\n\n\n\n\n","category":"type"},{"location":"refs/UnitPrefixedBytes/#PC2GPUBenchmarks.MB","page":"Unit-Prefixed Bytes","title":"PC2GPUBenchmarks.MB","text":"Megabytes, i.e. 10^6 = 1000^2 bytes\n\n\n\n\n\n","category":"type"},{"location":"refs/UnitPrefixedBytes/#PC2GPUBenchmarks.MiB","page":"Unit-Prefixed Bytes","title":"PC2GPUBenchmarks.MiB","text":"Mebibytes, i.e. 2^20 = 1024^2 bytes\n\n\n\n\n\n","category":"type"},{"location":"refs/UnitPrefixedBytes/#PC2GPUBenchmarks.TB","page":"Unit-Prefixed Bytes","title":"PC2GPUBenchmarks.TB","text":"Terabytes, i.e. 10^12 = 1000^4 bytes\n\n\n\n\n\n","category":"type"},{"location":"refs/UnitPrefixedBytes/#PC2GPUBenchmarks.TiB","page":"Unit-Prefixed Bytes","title":"PC2GPUBenchmarks.TiB","text":"Tebibytes, i.e. 2^40 = 1024^4 bytes\n\n\n\n\n\n","category":"type"},{"location":"refs/UnitPrefixedBytes/#PC2GPUBenchmarks.UnitPrefixedBytes","page":"Unit-Prefixed Bytes","title":"PC2GPUBenchmarks.UnitPrefixedBytes","text":"Abstract type representing an amount of data, i.e. a certain number of bytes, with a unit prefix (also \"metric prefix\"). Examples include the SI prefixes, like KB, MB, and GB, but also the binary prefixes (ISO/IEC 80000), like KiB, MiB, and GiB.\n\nSee https://en.wikipedia.org/wiki/Binary_prefix for more information.\n\n\n\n\n\n","category":"type"},{"location":"refs/UnitPrefixedBytes/#PC2GPUBenchmarks.bytes-Tuple{Number}","page":"Unit-Prefixed Bytes","title":"PC2GPUBenchmarks.bytes","text":"bytes(x::Number)\n\nReturns an appropriate UnitPrefixedBytes object, representing the number of bytes.\n\nNote: This function is type unstable by construction!\n\nSee simplify for what \"appropriate\" means here.\n\n\n\n\n\n","category":"method"},{"location":"refs/UnitPrefixedBytes/#PC2GPUBenchmarks.bytes-Tuple{UnitPrefixedBytes}","page":"Unit-Prefixed Bytes","title":"PC2GPUBenchmarks.bytes","text":"bytes(x::UnitPrefixedBytes)\n\nReturn the number of bytes (without prefix) as Float64.\n\n\n\n\n\n","category":"method"},{"location":"refs/UnitPrefixedBytes/#PC2GPUBenchmarks.change_base-Tuple{T} where T<:UnitPrefixedBytes","page":"Unit-Prefixed Bytes","title":"PC2GPUBenchmarks.change_base","text":"Toggle between\n\nBase 10, SI prefixes, i.e. factors of 1000\nBase 2, ISO/IEC prefixes, i.e. factors of 1024\n\nExample:\n\njulia> change_base(KB(13))\n~12.7 KiB\n\njulia> change_base(KiB(13))\n~13.31 KB\n\n\n\n\n\n","category":"method"},{"location":"refs/UnitPrefixedBytes/#PC2GPUBenchmarks.simplify-Tuple{B}","page":"Unit-Prefixed Bytes","title":"PC2GPUBenchmarks.simplify","text":"simplify(x::UnitPrefixedBytes[; base])\n\nGiven a UnitPrefixedBytes number x, finds a more appropriate UnitPrefixedBytes that represents the same number of bytes but with a smaller value.\n\nThe optional keyword argument base can be used to switch between base 2, i.e. ISO/IEC prefixes (default), and base 10. Allowed values are 2, 10, :SI, :ISO, and :IEC.\n\nNote: This function is type unstable by construction!\n\nExample:\n\njulia> simplify(B(40_000_000))\n~38.15 MiB\n\njulia> simplify(B(40_000_000); base=10)\n40.0 MB\n\n\n\n\n\n","category":"method"},{"location":"refs/gpuinfo/#GPU-Information","page":"GPU Information","title":"GPU Information","text":"","category":"section"},{"location":"refs/gpuinfo/","page":"GPU Information","title":"GPU Information","text":"Heavily inspired by the cuda sample deviceQueryDrv.","category":"page"},{"location":"refs/gpuinfo/#Index","page":"GPU Information","title":"Index","text":"","category":"section"},{"location":"refs/gpuinfo/","page":"GPU Information","title":"GPU Information","text":"Pages   = [\"gpuinfo.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/gpuinfo/#References","page":"GPU Information","title":"References","text":"","category":"section"},{"location":"refs/gpuinfo/","page":"GPU Information","title":"GPU Information","text":"Modules = [PC2GPUBenchmarks]\nPages   = [\"gpuinfo.jl\"]","category":"page"},{"location":"refs/gpuinfo/#PC2GPUBenchmarks.gpuinfo-Tuple{Integer}","page":"GPU Information","title":"PC2GPUBenchmarks.gpuinfo","text":"gpuinfo(deviceid::Integer)\n\nPrint out detailed information about the GPU with the given deviceid.\n\nHeavily inspired by the CUDA sample \"deviceQueryDrv.cpp\".\n\n\n\n\n\n","category":"method"},{"location":"refs/gpuinfo/#PC2GPUBenchmarks.gpuinfo-Tuple{}","page":"GPU Information","title":"PC2GPUBenchmarks.gpuinfo","text":"gpuinfo()\n\nShow general information about the available GPU related software and hardware. Among other things the versions of the CUDA toolkit, the CUDA driver, and the NVIDIA driver, as well as the available GPUs (names, total memory, etc.) will be printed.\n\nTo get more detailed information for a particular device, see gpuinfo(deviceid).\n\n\n\n\n\n","category":"method"},{"location":"refs/gpuinfo/#PC2GPUBenchmarks.gpuinfo_p2p_access-Tuple{}","page":"GPU Information","title":"PC2GPUBenchmarks.gpuinfo_p2p_access","text":"Query peer-to-peer (i.e. inter-GPU) access support.\n\n\n\n\n\n","category":"method"},{"location":"refs/hdf5/#HDF5","page":"HDF5","title":"HDF5","text":"","category":"section"},{"location":"refs/hdf5/#Index","page":"HDF5","title":"Index","text":"","category":"section"},{"location":"refs/hdf5/","page":"HDF5","title":"HDF5","text":"Pages   = [\"hdf5.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/hdf5/#References","page":"HDF5","title":"References","text":"","category":"section"},{"location":"refs/hdf5/","page":"HDF5","title":"HDF5","text":"Modules = [PC2GPUBenchmarks]\nPages   = [\"hdf5.jl\"]","category":"page"},{"location":"refs/hdf5/#PC2GPUBenchmarks.load_monitoring_results-Tuple{String}","page":"HDF5","title":"PC2GPUBenchmarks.load_monitoring_results","text":"Given an HDF5 file created with save_monitoring_results, restore the saved monitoring results (i.e. output of monitoring_stop).\n\n\n\n\n\n","category":"method"},{"location":"refs/hdf5/#PC2GPUBenchmarks.save_monitoring_results-Tuple{String, MonitoringResults}","page":"HDF5","title":"PC2GPUBenchmarks.save_monitoring_results","text":"save_monitoring_results(filename::String, r::MonitoringResults; overwrite=false)\n\nStore the given MonitoringResults (output of monitoring_stop) to disk as an HDF5 file with name filename.\n\n\n\n\n\n","category":"method"},{"location":"examples/peakflops_gpu/#Example:-Peakflops","page":"Peakflops","title":"Example: Peakflops","text":"","category":"section"},{"location":"examples/peakflops_gpu/","page":"Peakflops","title":"Peakflops","text":"note: Note\nTest system: NVIDIA A100 GPU","category":"page"},{"location":"examples/peakflops_gpu/#CUDA-cores","page":"Peakflops","title":"CUDA cores","text":"","category":"section"},{"location":"examples/peakflops_gpu/","page":"Peakflops","title":"Peakflops","text":"We assess the peak performance of the CUDA cores by executing many pure FMAs (fused multiply-adds) on the CUDA cores in parallel.","category":"page"},{"location":"examples/peakflops_gpu/","page":"Peakflops","title":"Peakflops","text":"Theoretically, we expect the performances below, which we compute from the clock rate, number of CUDA cores etc. The same numbers are given by NVIDIA, e.g. here or here, or Wikipedia, e.g. here.","category":"page"},{"location":"examples/peakflops_gpu/","page":"Peakflops","title":"Peakflops","text":"julia> theoretical_peakflops_gpu(; dtype=Float32, tensorcores=false);\nTheoretical Peakflops (TFLOP/s):\n ├ tensorcores: false\n ├ dtype: Float32\n └ max: 19.5\n\njulia> theoretical_peakflops_gpu(; dtype=Float64, tensorcores=false);\nTheoretical Peakflops (TFLOP/s):\n ├ tensorcores: false\n ├ dtype: Float64\n └ max: 9.7","category":"page"},{"location":"examples/peakflops_gpu/","page":"Peakflops","title":"Peakflops","text":"In good agreement, we find the following empirical numbers.","category":"page"},{"location":"examples/peakflops_gpu/","page":"Peakflops","title":"Peakflops","text":"julia> peakflops_gpu(; dtype=Float32, tensorcores=false);\nPeakflops (TFLOP/s):\n ├ tensorcores: false\n ├ dtype: Float32\n └ max: 19.1\n\njulia> peakflops_gpu(; dtype=Float64, tensorcores=false);\nPeakflops (TFLOP/s):\n ├ tensorcores: false\n ├ dtype: Float64\n └ max: 9.6\n\njulia> peakflops_gpu(; dtype=Float16, tensorcores=false);\nPeakflops (TFLOP/s):\n ├ tensorcores: false\n ├ dtype: Float16\n └ max: 12.8","category":"page"},{"location":"examples/peakflops_gpu/#Tensor-Cores","page":"Peakflops","title":"Tensor Cores","text":"","category":"section"},{"location":"examples/peakflops_gpu/","page":"Peakflops","title":"Peakflops","text":"We assess the peak performance of the Tensor cores by executing many pure WMMAs (warp-level matrix-multiply-and-accumulate), see, e.g., here and here), on the Tensor cores in parallel.","category":"page"},{"location":"examples/peakflops_gpu/","page":"Peakflops","title":"Peakflops","text":"Theoretically, we expect the performances below, which we compute from the clock rate, number of Tensor cores etc. The same numbers are given by NVIDIA, e.g. here or here, or on Wikipedia, e.g. here.","category":"page"},{"location":"examples/peakflops_gpu/","page":"Peakflops","title":"Peakflops","text":"julia> theoretical_peakflops_gpu(; dtype=Int8, tensorcores=true);\nTheoretical Peakflops (TOP/s):\n ├ tensorcores: true\n ├ dtype: Int8\n └ max: 623.7\n\njulia> theoretical_peakflops_gpu(; dtype=Float16, tensorcores=true);\nTheoretical Peakflops (TFLOP/s):\n ├ tensorcores: true\n ├ dtype: Float16\n └ max: 311.9\n\njulia> theoretical_peakflops_gpu(; dtype=Float32, tensorcores=true);\nTheoretical Peakflops (TFLOP/s):\n ├ tensorcores: true\n ├ dtype: Float32\n └ max: 155.9","category":"page"},{"location":"examples/peakflops_gpu/","page":"Peakflops","title":"Peakflops","text":"Empirically, we find the following numbers in good agreement.","category":"page"},{"location":"examples/peakflops_gpu/","page":"Peakflops","title":"Peakflops","text":"julia> peakflops_gpu(; dtype=Int8, tensorcores=true); # as of writing, only works with CUDA.jl#master\nPeakflops (TOP/s):\n ├ tensorcores: true\n ├ dtype: Int8\n └ max: 620.1\n\njulia> peakflops_gpu(; dtype=Float16, tensorcores=true);\nPeakflops (TFLOP/s):\n ├ tensorcores: true\n ├ dtype: Float16\n └ max: 311.2","category":"page"},{"location":"refs/workers/#Worker-Utilities","page":"Worker Utilities","title":"Worker Utilities","text":"","category":"section"},{"location":"refs/workers/#Index","page":"Worker Utilities","title":"Index","text":"","category":"section"},{"location":"refs/workers/","page":"Worker Utilities","title":"Worker Utilities","text":"Pages   = [\"workers.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/workers/#References","page":"Worker Utilities","title":"References","text":"","category":"section"},{"location":"refs/workers/","page":"Worker Utilities","title":"Worker Utilities","text":"Modules = [PC2GPUBenchmarks]\nPages   = [\"workers.jl\"]","category":"page"},{"location":"refs/workers/#PC2GPUBenchmarks.@worker-Tuple{Any, Any}","page":"Worker Utilities","title":"PC2GPUBenchmarks.@worker","text":"@worker pid ex\n\nSpawns the given command on the given worker process.\n\nExamples:\n\n@worker 3 PC2GPUBenchmarks.functional()\n@worker 3 stresstest(CUDA.devices(); duration=10, verbose=false)\n\n\n\n\n\n","category":"macro"},{"location":"refs/workers/#PC2GPUBenchmarks.@worker-Tuple{Any}","page":"Worker Utilities","title":"PC2GPUBenchmarks.@worker","text":"@worker ex\n\nCreates a worker process, spawns the given command on it, and kills the worker process once the command has finished execution.\n\nImplementation: a Julia thread (we use @spawn) will be used to wait on the task and kill the worker.\n\nExamples:\n\n@worker PC2GPUBenchmarks.functional()\n@worker stresstest(CUDA.devices(); duration=10, verbose=false)\n\n\n\n\n\n","category":"macro"},{"location":"refs/workers/#PC2GPUBenchmarks.@worker_create-Tuple{Any}","page":"Worker Utilities","title":"PC2GPUBenchmarks.@worker_create","text":"@worker_create n -> pids\n\nCreate n workers (i.e. separate Julia processes) and execute using PC2GPUBenchmarks, CUDA on all of them. Returns the pids of the created workers.\n\n\n\n\n\n","category":"macro"},{"location":"refs/workers/#PC2GPUBenchmarks.@worker_killall-Tuple{}","page":"Worker Utilities","title":"PC2GPUBenchmarks.@worker_killall","text":"Kills all Julia workers.\n\n\n\n\n\n","category":"macro"},{"location":"examples/host2device_bandwidth/#Example:-Host-to-Device-Memcpy","page":"Example: Host-to-Device Memcpy","title":"Example: Host-to-Device Memcpy","text":"","category":"section"},{"location":"examples/host2device_bandwidth/","page":"Example: Host-to-Device Memcpy","title":"Example: Host-to-Device Memcpy","text":"note: Note\nTest system: DGX with 8x A100 GPUs","category":"page"},{"location":"examples/host2device_bandwidth/","page":"Example: Host-to-Device Memcpy","title":"Example: Host-to-Device Memcpy","text":"julia> host2device_bandwidth()\nMemsize: 512.000 MiB\nGPU: CuDevice(0) - NVIDIA A100-SXM4-40GB\n\nHost <-> Device Bandwidth (GiB/s):\n └ max: 11.75\n\nHost (pinned) <-> Device Bandwidth (GiB/s):\n └ max: 24.24\n\nDevice <-> Device (same device) Bandwidth (GiB/s):\n └ max: 1226.84","category":"page"},{"location":"examples/host2device_bandwidth/","page":"Example: Host-to-Device Memcpy","title":"Example: Host-to-Device Memcpy","text":"Or with a bit more statistics on the benchmark runs:","category":"page"},{"location":"examples/host2device_bandwidth/","page":"Example: Host-to-Device Memcpy","title":"Example: Host-to-Device Memcpy","text":"julia> host2device_bandwidth(; stats=true)\nMemsize: 512.000 MiB\nGPU: CuDevice(0) - NVIDIA A100-SXM4-40GB\n\nHost <-> Device Bandwidth (GiB/s):\n ├ max: 11.71\n ├ min: 8.74\n ├ avg: 10.2\n └ std_dev: 1.22\n\nHost (pinned) <-> Device Bandwidth (GiB/s):\n ├ max: 24.15\n ├ min: 21.86\n ├ avg: 23.54\n └ std_dev: 0.74\n\nDevice <-> Device (same device) Bandwidth (GiB/s):\n ├ max: 1226.84\n ├ min: 1185.6\n ├ avg: 1220.56\n └ std_dev: 12.41","category":"page"},{"location":"refs/cuda_wrappers/#CUDA-Wrappers","page":"CUDA Wrappers","title":"CUDA Wrappers","text":"","category":"section"},{"location":"refs/cuda_wrappers/#Index","page":"CUDA Wrappers","title":"Index","text":"","category":"section"},{"location":"refs/cuda_wrappers/","page":"CUDA Wrappers","title":"CUDA Wrappers","text":"Pages   = [\"cuda_wrappers.md\"]\nOrder   = [:function, :type]","category":"page"},{"location":"refs/cuda_wrappers/#References","page":"CUDA Wrappers","title":"References","text":"","category":"section"},{"location":"refs/cuda_wrappers/","page":"CUDA Wrappers","title":"CUDA Wrappers","text":"Modules = [PC2GPUBenchmarks]\nPages   = [\"cuda_wrappers.jl\"]","category":"page"},{"location":"refs/cuda_wrappers/#PC2GPUBenchmarks.get_gpu_utilization","page":"CUDA Wrappers","title":"PC2GPUBenchmarks.get_gpu_utilization","text":"get_gpu_utilization(device=CUDA.device())\n\nGet the current utilization of the given CUDA device in percent.\n\n\n\n\n\n","category":"function"},{"location":"refs/cuda_wrappers/#PC2GPUBenchmarks.get_gpu_utilizations","page":"CUDA Wrappers","title":"PC2GPUBenchmarks.get_gpu_utilizations","text":"get_gpu_utilizations(devices=CUDA.devices())\n\nGet the current utilization of the given CUDA devices in percent.\n\n\n\n\n\n","category":"function"},{"location":"refs/cuda_wrappers/#PC2GPUBenchmarks.get_power_usage-Tuple{CUDA.NVML.Device}","page":"CUDA Wrappers","title":"PC2GPUBenchmarks.get_power_usage","text":"get_power_usage(device=CUDA.device())\n\nGet current power usage of the given CUDA device in Watts.\n\n\n\n\n\n","category":"method"},{"location":"refs/cuda_wrappers/#PC2GPUBenchmarks.get_power_usages","page":"CUDA Wrappers","title":"PC2GPUBenchmarks.get_power_usages","text":"get_power_usages(devices=CUDA.devices())\n\nGet current power usage of the given CUDA devices in Watts.\n\n\n\n\n\n","category":"function"},{"location":"refs/cuda_wrappers/#PC2GPUBenchmarks.get_temperature","page":"CUDA Wrappers","title":"PC2GPUBenchmarks.get_temperature","text":"get_temperature(device=CUDA.device())\n\nGet current temperature of the given CUDA device in degrees Celsius.\n\n\n\n\n\n","category":"function"},{"location":"refs/cuda_wrappers/#PC2GPUBenchmarks.get_temperatures","page":"CUDA Wrappers","title":"PC2GPUBenchmarks.get_temperatures","text":"get_temperatures(devices=CUDA.devices())\n\nGet current temperature of the given CUDA devices in degrees Celsius.\n\n\n\n\n\n","category":"function"},{"location":"refs/cuda_wrappers/#PC2GPUBenchmarks.gpuid","page":"CUDA Wrappers","title":"PC2GPUBenchmarks.gpuid","text":"Get GPU index of the given device.\n\nNote: GPU indices start with zero.\n\n\n\n\n\n","category":"function"},{"location":"examples/gpustresstest/#Example:-GPU-Stress-Test","page":"GPU Stress Test","title":"Example: GPU Stress Test","text":"","category":"section"},{"location":"examples/gpustresstest/","page":"GPU Stress Test","title":"GPU Stress Test","text":"note: Note\nTest system: DGX with 8x A100 GPUs","category":"page"},{"location":"examples/gpustresstest/#Single-GPU","page":"GPU Stress Test","title":"Single GPU","text":"","category":"section"},{"location":"examples/gpustresstest/","page":"GPU Stress Test","title":"GPU Stress Test","text":"julia> stresstest(device(); duration=10)\n[ Info: Will try to run for approximately 10 seconds on each GPU.\n[ Info: Running StressTest{Float32} on Julia thread 2 and CuDevice(0).\n[ Info: Ran 11241 iterations on CuDevice(0).\n[ Info: Clearing GPU memory.\n[ Info: Took 10.0 seconds to run the tests.","category":"page"},{"location":"examples/gpustresstest/#Multiple-GPUs","page":"GPU Stress Test","title":"Multiple GPUs","text":"","category":"section"},{"location":"examples/gpustresstest/","page":"GPU Stress Test","title":"GPU Stress Test","text":"First, make sure to run Julia with enough threads (more or equal to the number of GPU devices). Then, to run a GPU Stress test, use stresstest.","category":"page"},{"location":"examples/gpustresstest/","page":"GPU Stress Test","title":"GPU Stress Test","text":"In the following example, I started Julia with julia -t 20 (why not? 😄).","category":"page"},{"location":"examples/gpustresstest/","page":"GPU Stress Test","title":"GPU Stress Test","text":"julia> using PC2GPUBenchmarks\n\njulia> stresstest(CUDA.devices(); duration=10)\n[ Info: Will try to run for approximately 10 seconds on each GPU.\n[ Info: Running StressTest{Float32} on Julia thread 2 and CuDevice(0).\n[ Info: Running StressTest{Float32} on Julia thread 8 and CuDevice(6).\n[ Info: Running StressTest{Float32} on Julia thread 4 and CuDevice(2).\n[ Info: Running StressTest{Float32} on Julia thread 7 and CuDevice(5).\n[ Info: Running StressTest{Float32} on Julia thread 5 and CuDevice(3).\n[ Info: Running StressTest{Float32} on Julia thread 9 and CuDevice(7).\n[ Info: Running StressTest{Float32} on Julia thread 6 and CuDevice(4).\n[ Info: Running StressTest{Float32} on Julia thread 3 and CuDevice(1).\n[ Info: Ran 11222 iterations on CuDevice(3).\n[ Info: Ran 11203 iterations on CuDevice(6).\n[ Info: Ran 11227 iterations on CuDevice(2).\n[ Info: Ran 11230 iterations on CuDevice(4).\n[ Info: Ran 11231 iterations on CuDevice(1).\n[ Info: Ran 11199 iterations on CuDevice(0).\n[ Info: Ran 11215 iterations on CuDevice(5).\n[ Info: Ran 11246 iterations on CuDevice(7).\n[ Info: Clearing GPU memory.\n[ Info: Took 10.0 seconds to run the tests.","category":"page"},{"location":"examples/gpustresstest/#Monitoring","page":"GPU Stress Test","title":"Monitoring","text":"","category":"section"},{"location":"examples/gpustresstest/","page":"GPU Stress Test","title":"GPU Stress Test","text":"To verify that the GPUs are actually busy you could use external tools like, e.g., nvidia-smi. However, we also provide built-in monitoring tools.","category":"page"},{"location":"examples/gpustresstest/","page":"GPU Stress Test","title":"GPU Stress Test","text":"julia> using PC2GPUBenchmarks                                                       \n                                                                                    \njulia> monitoring_start()                                                           \n[ Info: Spawning monitoring on Julia thread 20.\n\njulia> stresstest(devices(); duration=10) # all devices, 10 seconds\n[ Info: Will try to run for approximately 10 seconds on each GPU.\n[ Info: Running StressTest{Float32} on Julia thread 4 and CuDevice(2).\n[ Info: Running StressTest{Float32} on Julia thread 2 and CuDevice(0).\n[ Info: Running StressTest{Float32} on Julia thread 6 and CuDevice(4).\n[ Info: Running StressTest{Float32} on Julia thread 3 and CuDevice(1).\n[ Info: Running StressTest{Float32} on Julia thread 9 and CuDevice(7).\n[ Info: Running StressTest{Float32} on Julia thread 7 and CuDevice(5).\n[ Info: Running StressTest{Float32} on Julia thread 5 and CuDevice(3).\n[ Info: Running StressTest{Float32} on Julia thread 8 and CuDevice(6).\n[ Info: Ran 11215 iterations on CuDevice(2).\n[ Info: Ran 11241 iterations on CuDevice(6).\n[ Info: Ran 11261 iterations on CuDevice(1).\n[ Info: Ran 11236 iterations on CuDevice(5).\n[ Info: Ran 11263 iterations on CuDevice(4).\n[ Info: Ran 11261 iterations on CuDevice(3).\n[ Info: Ran 11270 iterations on CuDevice(7).\n[ Info: Ran 11241 iterations on CuDevice(0).\n[ Info: Clearing GPU memory.\n[ Info: Took 10.0 seconds to run the tests.\n\njulia> results = monitoring_stop();\n[ Info: Stopping monitoring and fetching results...","category":"page"},{"location":"examples/gpustresstest/","page":"GPU Stress Test","title":"GPU Stress Test","text":"Here, results is a named tuple with time series information about GPU utilization, temperature, and more. See monitoring_stop for details.","category":"page"},{"location":"examples/gpustresstest/","page":"GPU Stress Test","title":"GPU Stress Test","text":"You can use save_monitoring_results to save the results on disk. We also provide plot_monitoring_results to visualize the results, which should give you an output like this.","category":"page"},{"location":"examples/gpustresstest/","page":"GPU Stress Test","title":"GPU Stress Test","text":"julia> plot_monitoring_results(results)\n\n             ⠀⠀⠀⠀⠀⠀⠀⠀⠀GPU Utilization (Compute)\n             ┌────────────────────────────────────────┐        \n         105 │⠀⠀⠀⠀⡤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⢤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 0: NVIDIA A100-SXM4-40GB\n             │⠀⠀⠀⢰⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 1: NVIDIA A100-SXM4-40GB\n             │⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 2: NVIDIA A100-SXM4-40GB\n             │⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 3: NVIDIA A100-SXM4-40GB\n             │⠀⠀⠀⡸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 4: NVIDIA A100-SXM4-40GB\n             │⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 5: NVIDIA A100-SXM4-40GB\n             │⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 6: NVIDIA A100-SXM4-40GB\n   U [%]     │⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 7: NVIDIA A100-SXM4-40GB\n             │⠀⠀⢰⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                             \n             │⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                             \n             │⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                             \n             │⠀⠀⡸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                             \n             │⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                             \n             │⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                             \n           0 │⣀⣀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                             \n             └────────────────────────────────────────┘                             \n             ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀20⠀                             \n             ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Time [s]⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀                                                          \n                                                                                                                 \n            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀GPU Temperature⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀                             \n            ┌────────────────────────────────────────┐                             \n         63 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 0: NVIDIA A100-SXM4-40GB\n            │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠤⠤⠤⠒⢒⣲⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 1: NVIDIA A100-SXM4-40GB\n            │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠤⠒⠊⣁⠤⣤⠤⠶⠮⠛⠛⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 2: NVIDIA A100-SXM4-40GB\n            │⠀⠀⠀⠀⠀⠀⠀⡠⣒⣉⣉⣭⣓⠭⠛⠋⠉⠉⠉⠀⠀⠀⠀⠀⠀⢻⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 3: NVIDIA A100-SXM4-40GB \n            │⠀⠀⠀⠀⠀⢠⣮⠮⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⢤⣤⠸⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 4: NVIDIA A100-SXM4-40GB \n            │⠀⠀⠀⠀⢠⡿⠁⠀⠀⠀⠀⠀⢀⡠⠤⣤⠤⠶⠮⠛⠋⣉⠭⠥⠤⡇⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 5: NVIDIA A100-SXM4-40GB \n            │⠀⠀⠀⢠⣷⠁⠀⣠⣒⠶⠒⢉⣉⢭⣛⣒⣒⡪⠝⠛⠛⠊⠉⠉⠉⣿⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 6: NVIDIA A100-SXM4-40GB \n   T [C]    │⠀⠀⠀⣼⠃⢠⠊⣁⠤⠔⠚⠓⠊⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⡞⠲⡤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 7: NVIDIA A100-SXM4-40GB \n            │⠀⠀⢠⡏⢠⣿⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⣧⠀⠈⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                              \n            │⠀⠀⣼⢡⣷⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                              \n            │⠤⠤⡟⣸⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⡳⡢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                              \n            │⠉⠛⢇⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠪⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                              \n            │⠀⠀⣸⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                              \n            │⠒⠒⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                              \n         28 │⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                              \n            └────────────────────────────────────────┘                              \n            ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀20⠀                              \n            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Time [s]⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀                              \n                                                                                                                 \n             ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀GPU Power Usage⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀                              \n             ┌────────────────────────────────────────┐                             \n         340 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣀⣀⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 0: NVIDIA A100-SXM4-40GB\n             │⠀⠀⠀⠀⣤⣠⣤⣤⣶⡶⠶⠶⠶⠾⠿⠿⣶⣿⣷⣶⣶⣶⣒⣒⣺⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 1: NVIDIA A100-SXM4-40GB\n             │⠀⠀⠀⢰⡯⡭⡿⠟⠛⠛⠛⠛⠛⠛⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠹⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 2: NVIDIA A100-SXM4-40GB\n             │⠀⠀⠀⢸⠋⡜⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 3: NVIDIA A100-SXM4-40GB\n             │⠀⠀⠀⢸⡰⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 4: NVIDIA A100-SXM4-40GB\n             │⠀⠀⠀⣾⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 5: NVIDIA A100-SXM4-40GB\n             │⠀⠀⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 6: NVIDIA A100-SXM4-40GB\n   P [W]     │⠀⠀⠀⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ GPU 7: NVIDIA A100-SXM4-40GB\n             │⠀⠀⢠⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                              \n             │⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                              \n             │⠀⠀⢸⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                              \n             │⠀⠀⣼⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢹⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                              \n             │⠀⠀⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                              \n             │⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣶⣶⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                              \n          56 │⣶⣶⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠙⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│                              \n             └────────────────────────────────────────┘                              \n             ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀20⠀                              \n             ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀Time [s]⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀                              ","category":"page"},{"location":"examples/memory_bandwidth/#Example:-Memory-Bandwidth-(SAXPY-GPU)","page":"Example: Memory Bandwidth (SAXPY GPU)","title":"Example: Memory Bandwidth (SAXPY GPU)","text":"","category":"section"},{"location":"examples/memory_bandwidth/","page":"Example: Memory Bandwidth (SAXPY GPU)","title":"Example: Memory Bandwidth (SAXPY GPU)","text":"note: Note\nTest system: DGX with 8x A100 GPUs","category":"page"},{"location":"examples/memory_bandwidth/","page":"Example: Memory Bandwidth (SAXPY GPU)","title":"Example: Memory Bandwidth (SAXPY GPU)","text":"julia> memory_bandwidth();\nMemory Bandwidth (GiB/s):\n └ max: 1192.09\n\njulia> memory_bandwidth(; cublas=false); # use a custom saxpy kernel\nMemory Bandwidth (GiB/s):\n └ max: 1179.8","category":"page"},{"location":"examples/memory_bandwidth/","page":"Example: Memory Bandwidth (SAXPY GPU)","title":"Example: Memory Bandwidth (SAXPY GPU)","text":"julia> memory_bandwidth_scaling();\n\n              ⠀⠀Peak: 1273.52 GiB/s (size = 304087040)⠀⠀ \n              ┌────────────────────────────────────────┐ \n         1280 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⢀⣀⣀⡠⠤⠤⠔⠒⠒⠢⠊⠊⢹│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣠⠤⠔⠁⠈⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣠⠔⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠎⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡰⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠜⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n   GiB/s      │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡜⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡎⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⠀⡸⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⠀⠀⠀⢠⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              │⠀⠀⠀⠀⠀⢀⣀⡠⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n         1190 │⠤⠔⠒⠊⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸│ \n              └────────────────────────────────────────┘ \n              ⠀2²³⸱³²¹⁹⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀2²⁸⸱²²⁸⁸⠀ \n              ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀vector length⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ","category":"page"},{"location":"#PC2GPUBenchmarks.jl","page":"PC2GPUBenchmarks","title":"PC2GPUBenchmarks.jl","text":"","category":"section"},{"location":"#Installation","page":"PC2GPUBenchmarks","title":"Installation","text":"","category":"section"},{"location":"","page":"PC2GPUBenchmarks","title":"PC2GPUBenchmarks","text":"The package is not registered in the General registry, but only in the PC2Registry. If you have subscribed to the latter, you can simply add the package as follows.","category":"page"},{"location":"","page":"PC2GPUBenchmarks","title":"PC2GPUBenchmarks","text":"] add PC2GPUBenchmarks.jl","category":"page"},{"location":"","page":"PC2GPUBenchmarks","title":"PC2GPUBenchmarks","text":"Otherwise, you can readily add it by using the explicit URL.","category":"page"},{"location":"","page":"PC2GPUBenchmarks","title":"PC2GPUBenchmarks","text":"] add https://git.uni-paderborn.de/pc2/julia/PC2GPUBenchmarks.jl","category":"page"},{"location":"","page":"PC2GPUBenchmarks","title":"PC2GPUBenchmarks","text":"Note: The minimal required Julia version is 1.6 but we recommend using Julia ≥ 1.7.","category":"page"},{"location":"#Getting-Started","page":"PC2GPUBenchmarks","title":"Getting Started","text":"","category":"section"},{"location":"","page":"PC2GPUBenchmarks","title":"PC2GPUBenchmarks","text":"Pages = map(file -> joinpath(\"examples\", file), readdir(\"examples\"))\nDepth = 1","category":"page"},{"location":"explanations/dgx/#DGX-A100","page":"DGX Details","title":"DGX-A100","text":"","category":"section"},{"location":"explanations/dgx/","page":"DGX Details","title":"DGX Details","text":"Information by NVIDIA is available here.","category":"page"},{"location":"explanations/dgx/#Topology","page":"DGX Details","title":"Topology","text":"","category":"section"},{"location":"explanations/dgx/","page":"DGX Details","title":"DGX Details","text":"(Image: topology)","category":"page"},{"location":"explanations/dgx/","page":"DGX Details","title":"DGX Details","text":"(Image source: https://www.microway.com/wp-content/uploads/NVIDIA-DGX-A100-Block-Diagram.png","category":"page"}]
}
